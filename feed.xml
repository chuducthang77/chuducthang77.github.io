<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://chuducthang77.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://chuducthang77.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-22T06:34:39+00:00</updated><id>https://chuducthang77.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Resources</title><link href="https://chuducthang77.github.io/blog/2023/resources/" rel="alternate" type="text/html" title="Resources"/><published>2023-02-03T00:00:00+00:00</published><updated>2023-02-03T00:00:00+00:00</updated><id>https://chuducthang77.github.io/blog/2023/resources</id><content type="html" xml:base="https://chuducthang77.github.io/blog/2023/resources/"><![CDATA[<h1 id="latest-updated-jan-2-2024">Latest updated: Jan 2, 2024</h1> <h1 id="research-utilities">Research utilities</h1> <h2 id="bookssurvey">Books/Survey</h2> <ul> <li><a href="https://www.cs197.seas.harvard.edu/">Harvard CS197 - AI research experiences</a></li> <li><a href="https://web.stanford.edu/class/cs197/">Stanford CS197 - Computer Science research</a></li> <li><a href="https://csresearch101.netlify.app/m3.html">CS Research 101 by Neeldhara Misra and Shashank Srikant</a></li> <li><a href="https://www.let-all.com/assets/slides/How-to-ALT22-Aaditya.pdf">How to read research papers by Aaditya Ramdas</a></li> <li><a href="https://docs.google.com/document/d/1QREmdzLwJ0CR3kdFeenJbBowT1IFFREd46y10tW6pog/edit#heading=h.ugwg0glmykz5">AI research journey and advice by Jason Wei</a></li> <li><a href="https://mathweb.ucsd.edu/~fan/teach/YouAndYourResearch.pdf">You and Your research by Richard Hamming</a></li> <li><a href="https://mathweb.ucsd.edu/~fan/teach/gradpol.html">A few words on research for graduate students by Fan Chung Graham</a></li> <li><a href="https://www.nature.com/articles/426389a.pdf">Four golden lessons by Steven Weinberg</a></li> <li><a href="https://davidstutz.de/how-i-prepared-for-deepmind-and-google-ai-research-internship-interviews-in-2019/">How I prepared for Deepmind and google AI research internship interviews in 2019 by David Stutz</a></li> <li><a href="https://www.alextamkin.com/essays/tips-for-new-researchers">Tips for Success as a New Researcher by Alex Tamkin</a></li> <li><a href="https://cs.stanford.edu/~jsteinhardt/ResearchasaStochasticDecisionProcess.html">Research as a Stochastic Decision Process by Jacob Steinhardt</a></li> <li><a href="https://cacm.acm.org/magazines/2013/7/165494-phd-students-must-break-away-from-undergraduate-mentality/abstract">Ph.D students must break away from undergraduate mentality by Jason Hong</a></li> <li><a href="https://colah.github.io/notes/taste/">Research Taste Exercises by Christopher Olah</a></li> <li><a href="https://danieltakeshi.github.io/2018/12/17/better-logging/">Better Saving and Logging for Research experiments by Daniel Seita</a></li> <li><a href="https://cs-sop.org/">CS PhD Statements of Purpose</a></li> <li><a href="https://suchin.io/personal-statement-advice/">Personal Statement Advice by Suchin Gururangan</a></li> <li><a href="https://blog.nelsonliu.me/2020/11/11/phd-personal-statement/">PhD Statement of Purpose by Nelson Liu</a></li> <li><a href="https://www.ekzhang.com/writing">Writing by Eric Zhang</a></li> <li><a href="https://ai.papers.bar/feed/recent">AI paper Feed</a></li> <li><a href="https://cs.stanford.edu/people/widom/paper-writing.html#versions">Tips for Writing Technical Papers by Jennifer Widom</a></li> <li><a href="https://arxiv.org/pdf/2108.02497.pdf">How to avoid ML pitfalls: a guide for academic researchers by Michael A. Lones</a></li> <li><a href="https://www.cs.jhu.edu/~jason/advice/write-the-paper-first.html?ref=ruder.io">Write the paper first by Jason Eisner</a></li> <li><a href="http://joschu.net/blog/opinionated-guide-ml-research.html">An opinionated guide to ML research by John Schulman</a></li> <li><a href="https://www.andreykurenkov.com/writing/life/lessons-learned-from-failures/?ref=ruder.io">Lessons learned the hard way in grad school (so far) by Andrey Kurenkov</a></li> <li><a href="https://bigaidream.gitbooks.io/tech-blog/content/2014/de-mystifying-good-research.html">De-Mystifying Good Research and Good Papers by Fei-Fei Li</a></li> <li><a href="https://calnewport.com/what-you-know-matters-more-than-what-you-do/">What You Know Matters More Than What You Do by Cal Newport</a></li> <li><a href="https://colah.github.io/posts/2019-05-Collaboration/?ref=ruder.io">Collaboration and Credit principles by Christopher Olah</a></li> <li><a href="https://danieltakeshi.github.io/2013/01/17/the-phd-grind-philip-guos-e-book/">The PhD Grind — Philip Guo’s E-book bu Daniel Takeshi</a></li> <li><a href="https://rishabh-ranjan.github.io/assets/phd_sop.pdf">SOP - Rishabh Ranjan</a></li> <li><a href="https://docs.google.com/document/d/1QKInEBpoIWMGTZeDhZ8nI-TkJuSRr5YCD23-zMMlYGA/edit#heading=h.hjb46h7zelmo">SOP - Ameya Daigavane</a></li> <li><a href="https://drive.google.com/file/d/1DgXqEC2DFhEYAHMFWJroYfMgKHbjwy7u/view">SOP - Siddartha Devic</a></li> <li><a href="https://docs.google.com/document/d/1d3bhFQ1e5oOmSZ1H5Cgvf5sd7L9M00zkYhdPl4UPvfg/edit">SOP - Aaron Dharna</a></li> <li><a href="https://naveenraman.com/assets/sop/CMU_ML_Statement_of_Purpose.pdf">SOP - Naveen Raman</a></li> <li><a href="https://matt.might.net/articles/successful-phd-students/">3 qualities of successful Ph.D. students; Perseverance, tenacity and cogency by Matt Might</a></li> <li><a href="https://matt.might.net/articles/console-hacks-exploiting-frequency/">Console productivity hack: Discover the frequent; then make it the easy by Matt Might</a></li> <li><a href="https://matt.might.net/articles/nine-kinds-of-students/">Classroom Fortress: The Nine Kinds of Students by Matt Might</a></li> <li><a href="https://matt.might.net/articles/ways-to-fail-a-phd/">10 easy ways to fail a Ph.D. by Matt Might</a></li> <li><a href="https://matt.might.net/articles/productivity-tips-hints-hacks-tricks-for-grad-students-academics/#laptop">Productivity tips, tricks and hacks for academics (2015 edition) by Matt Might</a></li> <li><a href="https://matt.might.net/articles/what-cs-majors-should-know/">What every computer science major should know by Matt Might</a></li> <li><a href="https://fanpu.io/blog/">Fan Pu Zeng</a></li> <li><a href="https://arxiv.org/pdf/1807.03341.pdf">Troubling trends in machine learning scholarship by Zachary Lipton, Jacob Steinhardt</a></li> <li><a href="https://thmatters.wordpress.com/funding-opportunities-and-tips/career-examples-proposalscomments/">Career examples: proposals+comments</a></li> <li><a href="https://www.cs.jhu.edu/~jason/advice/how-to-organize-your-files.html">How to organize your files by Jason Eisner</a></li> <li><a href="https://www.cs.jhu.edu/~jason/advice/how-to-find-research-problems.html">How to find research problems by Jason Eisner</a></li> <li><a href="https://matt.might.net/articles/shell-scripts-for-passive-voice-weasel-words-duplicates/">3 shell scripts to improve your writing, or “My Ph.D. advisor rewrote himself in bash by Matt Might</a></li> <li><a href="https://link.springer.com/book/10.1007/b97682">Problem-Solving strategies by Arthur Engel</a> <h2 id="links">Links</h2> </li> <li><a href="https://github.com/google-research/tuning_playbook">Tuning playbook</a></li> <li><a href="https://github.com/jbhuang0604/awesome-tips">Jia-Huang Bin research advice</a></li> <li><a href="https://research.mlcontests.com/">ML Contests</a></li> <li><a href="https://arxiv-sanity-lite.com/">Arxiv-sanity</a></li> <li><a href="https://www.cs.cmu.edu/academics/fellowships/">Graduate Fellowship Opportunities 2023-24 Academic Year</a></li> <li><a href="https://github.com/eugeneyan/applied-ml">Applied ML</a></li> <li><a href="https://greatresearch.org/">How to Do Great Research</a> <h2 id="bloggers">Bloggers</h2> </li> <li><a href="https://ruder.io/">Sebastian Ruder</a> - NLP-focused</li> <li><a href="https://lilianweng.github.io/lil-log/">Lil’Log</a>: Many introductions to new topics, well-written, easy to understand, highly recommend!</li> <li><a href="https://jalammar.github.io/">Jay Alammar</a>: Many introductions to new topics, good visualizations</li> <li><a href="http://karpathy.github.io/">Andrej Karpathy</a>: Legends in the NLP, many good tips</li> <li><a href="https://distill.pub/">Distill</a> Many introductions to new topics, good visualizations</li> <li><a href="http://colah.github.io/">Colah’s blog</a>: Focus on computer vision, many good tips, collaboration with Distill.</li> <li><a href="https://blogs.princeton.edu/imabandit/about-me/">I’m a bandit</a>: Optimization, statistics, probability theory, ML theory.</li> <li><a href="https://sudeepraja.github.io/">Sudeep Raja</a>: Online learning</li> <li><a href="https://bounded-regret.ghost.io">Bounded Regret</a>: Introductions to new topics, many opinions, many tips</li> <li><a href="https://www.inference.vc/">inFERENCe</a>: Statistics, various topics, many tutorials, insights and opinions</li> <li><a href="https://bost.ocks.org/mike/">Mike Bostock</a>: Software tips</li> <li><a href="https://michaelnielsen.org/">Michael Nielsen</a>: Focused on CV, touch on quantum.</li> <li><a href="https://danieltakeshi.github.io/new-start-here.html">Daniel Takeshi’s blog</a> Various introductory topics, class reviews @Berkeley, lots of tips</li> <li><a href="http://gregorygundersen.com/blog/">Gregory Gundersen</a>: Focused on statistics, time-series data, GP, Bayesian inference</li> <li><a href="https://mpatacchiola.github.io/blog/">Massimiliano (Max) Patacchiola’s blog</a>: RL, few-shot, variational inference</li> <li><a href="https://francisbach.com/">Machine Learning Research Blog – Francis Bach</a>: Focused on theoretical ML</li> <li><a href="https://www.argmin.net/">arg min blog</a>: Focused on theoretical ML, a lot of experience</li> <li><a href="https://kiranvodrahalli.github.io/links/#resources-notes-textbooks-monographs-classes-etc">Kiran Vodrahalli</a>: Huge resources</li> <li><a href="https://machinethoughts.wordpress.com/">Machine Thoughts</a> : Various topics, lots of opinions, AI general</li> <li><a href="https://www.offconvex.org/">Off convex</a>: ML theory</li> <li><a href="https://hunch.net/">Hunch</a>: ML theory</li> <li><a href="https://www.stat.cmu.edu/~cshalizi/">Cosma Rohilla Shalizi</a>: CMU statistics prof, very good notebooks</li> <li><a href="https://windowsontheory.org/">Windows on theory</a>: Original thought on many topics</li> <li><a href="https://karlstratos.com/#notes">Karl Stratos</a></li> <li><a href="https://blog.ml.cmu.edu/category/educational/">Machine Learning Blog | ML@CMU </a></li> <li><a href="http://fastml.com/blog/page/2/">FastML</a></li> <li><a href="https://openai.com/blog/">OpenAI Blog</a></li> <li><a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog – Medium</a></li> </ul> <h1 id="math-ml">Math ML</h1> <h2 id="courses">Courses</h2> <ul> <li><a href="https://www.cs.cmu.edu/~10606-f21/">CMU 10606 - Math ML</a></li> <li><a href="https://www.cs.cmu.edu/~odonnell/papers/cs-theory-toolkit-lecture-notes.pdf">Theoretical Toolkit for CS</a></li> <li><a href="https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227BT/index.html">Berkeley EE227BT - Convex Optimization</a></li> <li><a href="https://ee227c.github.io/">Berkeley EE227C - Convex optimization and approximation</a></li> <li><a href="https://people.eecs.berkeley.edu/~elghaoui/Teaching/EECS127/index.html">Berkeley EECS 127/227 AT - Optimization models and applications</a></li> <li><a href="https://www.cs.cornell.edu/courses/cs4783/2022sp/">Cornell CS 4783/5783 Mathematical foundations of machine learning</a></li> <li><a href="https://aaa.princeton.edu/orf523">Princeton ORF523 - Convex and Conic optimization</a></li> <li><a href="https://yuxinchen2020.github.io/ele522_optimization/">Princeton ELE522 - Large-Scale Optimization for Data Science</a></li> <li><a href="https://people.orie.cornell.edu/dsd95/orie6300.html">Cornell ORIE 6300 - Mathematical Programming I</a></li> <li><a href="https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15859-f11/www/">CMU 15-859(E) - Linear and Semidefinite Programming (Advanced Algorithms)</a></li> <li><a href="https://www.math.cmu.edu/~gautam/sj/teaching/2022-23/720-measure/">CMU MATH 720 - Measure Theory and Integration</a></li> <li><a href="https://cds.nyu.edu/math-tools/">NYU Mathematical tools for data science</a></li> <li><a href="https://home.ttic.edu/~avrim/Toolkit23/">TTIC 31150/CMSC 31150 - Mathematical Toolkit</a></li> <li><a href="https://armahmood.github.io/340w22/">CMPUT 340 - Introduction to Numerical Methods</a> <h2 id="bookssurvey-1">Books/Survey</h2> </li> <li><a href="https://www.cis.upenn.edu/~jean/math-deep.pdf">Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning by Jean Gallier and Jocelyn Quaintance</a></li> <li><a href="https://gwthomas.github.io/docs/math4ml.pdf">Mathematics for Machine Learning -UC-Berkeley by Garrett Thomas</a></li> <li><a href="https://realnotcomplex.com/">Real Not Complex</a></li> <li><a href="https://59clc.files.wordpress.com/2011/01/real-and-complex-analysis.pdf">Real and Complex analysis by Walter Rudin</a></li> <li><a href="http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf">Real Analysis: Measure Theory, Integration and Hilbert spaces by Elias M. Stein and Rami Shakarchi</a></li> <li><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">Matrix Cookbook by Kaare Brandt Petersen and Michael Syskind Pedersen</a></li> <li><a href="https://www.cs.cmu.edu/~zkolter/course/linalg/">Linear algebra review by Zico Kolter</a></li> <li><a href="https://sites.ualberta.ca/~kashlak/data/stat571.pdf">Probability and Measure - Course notes for STAT 571</a></li> <li><a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity by Sébastien Bubeck</a></li> <li><a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">Introductory Lectures on Convex Optimization: A Basic Course by Yurii Nesterov</a></li> <li><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization by Stephen Boyd, Lieven Vandenberghe</a></li> <li><a href="https://www.cambridge.org/core/books/all-the-math-you-missed/02DEDEA470A50F689C9686D835108456">All the math you missed (but need to know for graduate school) by Thomas Garrity</a></li> <li><a href="https://mml-book.github.io/book/mml-book.pdf">Mathematics for Machine Learning by Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong</a></li> <li><a href="https://kiranvodrahalli.github.io/notes/340_notes.pdf">Group theory by Mark McConnell</a></li> <li><a href="https://www.amazon.com/Topology-2nd-James-Munkres/dp/0131816292">Topology by James Munkres</a></li> <li><a href="https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf">Proximal algorithms by Neal Parikh, Stephen Boyd</a> <h2 id="papers">Papers</h2> </li> <li><a href="https://arxiv.org/pdf/1802.01528.pdf">Matrix Calculus You need for DL by Terence Parr and Jeremy Howard</a></li> <li><a href="https://arxiv.org/pdf/2203.08890.pdf">The math of AI by Gitta Kutyniok</a></li> <li><a href="https://arxiv.org/pdf/2105.04026.pdf">The modern math of DL by Julius Berner, Philipp Grohs, Gitta Kutyniok, Philipp Petersen</a></li> </ul> <h1 id="computational-ml">Computational ML</h1> <h2 id="courses-1">Courses</h2> <ul> <li><a href="https://www.cs.cmu.edu/~10607-f21/">CMU 10607 - Computational ML</a></li> <li><a href="https://www.probabilistic-numerics.org/teaching/2022_Numerics_of_Machine_Learning/">Numerics of Machine Learning</a> <h2 id="bookssurvey-2">Books/Survey</h2> <h2 id="papers-1">Papers</h2> </li> </ul> <h1 id="deep-learning">Deep Learning</h1> <h2 id="courses-2">Courses</h2> <ul> <li><a href="https://mcallester.github.io/ttic-31230/">TTIC 31230 - Fundamentals of DL</a></li> <li><a href="https://andrejristeski.github.io/10417-20/index.html#announcement">CMU 10417 - Intermediate Deep Learning</a></li> <li><a href="https://dlsyscourse.org/lectures/">CMU 10414 - Deep Learning System</a></li> <li><a href="https://deeplearning-cmu-10707-2022spring.github.io/index.html#logistics">CMU 10707 - Deep Learning</a></li> <li><a href="https://mjt.cs.illinois.edu/dlt/">Illinois - DL theory</a></li> <li><a href="https://www.cs.princeton.edu/courses/archive/fall19/cos597B/">Princeton CS597B - Theoretical DL</a></li> <li><a href="https://www.cs.umd.edu/class/fall2020/cmsc828W/index.html">Uni Maryland CMSC 828W - Foundations of DL</a></li> <li><a href="https://stats385.github.io/">Stanford STATS 385 - Analyses of DL</a></li> <li><a href="https://cs182sp21.github.io/">CS W182/282 - Designing, visualizing, and understanding deep neural networks</a></li> <li><a href="http://csg.csail.mit.edu/6.5930/info.html">MIT 65930 - Hardware architecture for DL</a></li> <li><a href="https://stats385.github.io/">Stanford STATS385 - Analyses of Deep Learning</a></li> <li><a href="https://joanbruna.github.io/stat212b/">UC Berkeley Stat212b - Topics Course on Deep Learning</a></li> <li><a href="http://elmos.scripts.mit.edu/mathofdeeplearning/mathematical-aspects-of-deep-learning-intro/">MIT 18.177 - Mathematical Aspects of Deep Learning</a></li> <li><a href="https://stellar.mit.edu/S/course/6/sp18/6.883/materials.html">MIT 6.883 - Science of Deep Learning: Bridging Theory and Practice</a> <h2 id="bookssurvey-3">Books/Survey</h2> </li> <li><a href="https://www.deeplearningbook.org/">Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville</a></li> <li><a href="http://neuralnetworksanddeeplearning.com/">Neural network and Deep Learning by Michael Nielsen</a></li> <li><a href="https://fleuret.org/public/lbdl.pdf">The Little Book of Deep Learning by Francois Fleuret</a></li> <li><a href="https://mjt.cs.illinois.edu/dlt/">Deep learning theory lecture notes by Matus Telgarsky</a></li> <li><a href="https://drive.google.com/file/d/1GKf_V8s2jUMIrtMUwb4i8kpOPzr_QlEU/view">Challenges in DL by Razvan Pascanu</a></li> <li><a href="https://arxiv.org/pdf/2106.10165.pdf">The principles of DL Theory by Daniel A. Roberts and Sho Yaida</a></li> <li><a href="https://www.cs.princeton.edu/courses/archive/fall19/cos597B/lecnotes/bookdraft.pdf">Theory of deep learning by Raman Arora, Sanjeev arora, Joan Bruna, Nadav Cohen, Rong Ge, Suriya Gunasekar, Chi Jin, Jason Lee, Tengyu Ma, Behnam Neyshabur, Zhao Song</a></li> <li><a href="https://arxiv.org/pdf/2103.09177.pdf">Deep learning: a statistical viewpoint by Peter Bartlett, Andrea Montanari, Alexander Rakhlin</a></li> <li><a href="https://arxiv.org/pdf/2310.20360.pdf">Mathematical introduction to deep learning: methods, implementations, and theory by Arnulf Jentzen, Benno Kuckuck, Philippe von Wurstemberger</a> <h2 id="papers-2">Papers</h2> </li> <li><a href="https://arxiv.org/pdf/2301.11316.pdf">Open Problems in Applied Deep Learning by Maziar Raissi</a></li> <li><a href="https://danieltakeshi.github.io/2018/12/17/better-logging/">A statistician teaches deep learning by G. Jogesh Babu, David Banks, Hyunsoon Cho, David Han, Hailin Sang, Shouyi Wang</a></li> <li><a href="https://www.dropbox.com/s/qonozmne0x4x2r3/deepsurveyICML18final.pptx?dl=0">Toward Theoretical Understanding of DL by Sanjeev Arora</a></li> <li><a href="https://fengxianghe.github.io/paper/he2020recent.pdf">Recent advances in deep learning theory by Fengxiang He, Dacheng Tao</a> <h1 id="machine-learning">Machine Learning</h1> <h2 id="courses-3">Courses</h2> </li> <li><a href="https://www.cs.cmu.edu/~pradeepr/courses/716/2019-spring/">CMU 10716 - Advanced Machine Learning</a></li> <li><a href="https://www.cs.cornell.edu/courses/cs6780/2019sp/">Cornell CS6780 - Advanced Machine Learning</a></li> <li><a href="https://1five9.github.io/">Caltech CS159 - Advanced ML</a></li> <li><a href="https://web.stanford.edu/class/stats214/">Stanford STATS214/CS229M - Machine Learning Theory</a></li> <li><a href="https://www.cs.cornell.edu/courses/cs6783/2022fa/">Cornell CS6783 - Machine Learning Theory</a></li> <li><a href="https://www.cs.princeton.edu/courses/archive/spring18/cos511/schedule.html">Princeton CS 511 - Theoretical ML</a></li> <li><a href="https://mltheorycourse.github.io/pages/about/">UofA CMPUT 654 - Theoretical foundations of ML</a></li> <li><a href="https://home.ttic.edu/~avrim/MLT22/">TTIC 31250 - An Introduction to the Theory of Machine Learning</a></li> <li><a href="https://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/">CMUT 10715 - Advanced Intro to ML</a></li> <li><a href="https://10605.github.io/#schedule">CMU 10605 - ML with large datasets</a></li> <li><a href="https://www.cs.cmu.edu/~mgormley/courses/10418/index.html">CMU 10418 - ML for structured data</a></li> <li><a href="https://abdelfattah-class.github.io/ece5545/">Cornell ECE 5545 - ML hardware and systems</a></li> <li><a href="https://cs229s.stanford.edu/fall2023/">Stanford CS 229s - System for ML</a></li> <li><a href="https://www.youtube.com/@gaussianprocesssummerschoo7738">Gaussian Process Summer School</a></li> <li><a href="https://pages.github.berkeley.edu/UCB-EECS208/course_site/">Berkeley EECS 208 - Computational Principles for High-Dimensional Data Analysis</a></li> <li><a href="https://www.stat.cmu.edu/~larry/=sml/">CMU 36708 - Statistical methods for machine learning</a></li> <li><a href="https://homes.cs.washington.edu/~sham/courses/stat928/index.html">Washington STAT 928 - Statistical Learning theory</a></li> <li><a href="https://home.ttic.edu/~nati/Teaching/TTIC31120/2016/">TTIC 31120: Computational and Statistical Learning Theory</a></li> <li><a href="https://www.mit.edu/~9.520/fall18/">MIT 9.520/6.860 - Statistical Learning Theory and Applications</a></li> <li><a href="https://www.overleaf.com/project/5e1c8cbfcf9ddb0001f14b29">CMU 36708 - The ABCDE of Statistical methods for ML</a></li> <li><a href="https://home.ttic.edu/~nati/Teaching/TTIC31120/2016/">TTIC 31120: Computational and Statistical Learning Theory</a></li> <li><a href="https://www.cs.columbia.edu/~cs4252/">Columbia COMS 4252 - Intro to computational learning theory</a></li> <li><a href="https://www.stat.cmu.edu/~cshalizi/sml/21/">CMU 36465/665 - Conceptual Foundations of Statistical Learning</a></li> <li><a href="http://tensorlab.cms.caltech.edu/users/anima/cms165-2020.html#lectures">Caltech CS/CNS/EE/IDS 165 - Foundations of Machine Learning and Statistical Inference</a></li> <li><a href="https://www.cs.cmu.edu/~dwoodruf/teaching/15859-fall22/index.html">CMU 15859 - Algorithms for Big Data</a></li> <li><a href="https://hands-on-sgd.readthedocs.io/en/latest/">A Hands-on Approach for Implementing Stochastic Optimization Algorithms from Scratch</a></li> <li><a href="https://adaptivedataanalysis.com/">UPenn - The algorithmic foundations of adaptive data analysis</a></li> <li><a href="https://people.csail.mit.edu/moitra/408.html">MIT 18408 - Algorithmic Aspects of ML</a></li> <li><a href="https://www.cs.cornell.edu/courses/cs6781/2020sp/">Cornell 6781 - Foundations of Modern Machine Learning</a></li> <li><a href="https://www.cs.columbia.edu/~djhsu/coms4772-f16/index.html">Columbia COMS 4772 - ML Theory</a></li> <li><a href="https://www.cs.columbia.edu/~verma/classes/uml/index.html">Columbia COMS 4774 - Unsupervised Learning</a></li> <li><a href="https://www.cs.princeton.edu/courses/archive/spring17/cos598E/">Princeton COS 598 - Unsupervised Learning: Theory and Practice</a></li> <li><a href="http://ciml.info/">A course in ML by Hal Daume</a></li> <li><a href="https://courses.cs.duke.edu//fall16/compsci590.2/">Duke COMPSCI 590.2 - Algorithimic Aspects of Machine Learning</a></li> <li><a href="https://www.cs.princeton.edu/courses/archive/fall17/cos597A/">Princeton CS 597A - New Directions in Theoretical Machine Learning</a></li> <li><a href="https://simons.berkeley.edu/programs/foundations-machine-learning">Simons - Foundations of Machine Learning</a></li> <li><a href="https://simons.berkeley.edu/programs/foundations-data-science">Simons - Foundations of Data Science</a></li> <li><a href="https://mva-kernel-methods.github.io/course-2021-2022/">Machine Learning with kernel methods</a></li> <li><a href="https://timroughgarden.org/f13/f13.html">Stanford CS364A - Algorithmic Game Theory</a></li> <li><a href="https://web.eecs.umich.edu/~jabernet/eecs598course/fall2015/web/">Michigan EECS 598 - Theoretical foundations of ML</a> <h2 id="bookssurvey-4">Books/Survey</h2> </li> <li><a href="https://arxiv.org/abs/2206.13446">Pen and Paper Exercises in ML by Michael U. Gutmann</a></li> <li><a href="https://mlstory.org/">Patterns, Predictions, and Actions - A story about machine learning by Moritz Hardt and Benjamin Recht</a></li> <li><a href="https://tongzhang-ml.org/lt-book/lt-book.pdf">Mathematical Analysis of Machine Learning Algorithms by Tong Zhang</a></li> <li><a href="https://aima.cs.berkeley.edu/">Artificial Intelligence: A modern approach by Stuart Russell and Peter Norvig</a></li> <li><a href="https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning">Deep Learning Cheatsheet by Afshine Amidi and Shervine Amidi</a></li> <li><a href="https://bayesopt-tutorial.github.io/syllabus/fullslides.pdf">Recent Advances in Bayesian Optimization</a></li> <li><a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">The Algorithmic Foundations of Differential Privacy by Cynthia Dwork, Aaron Roth</a></li> <li><a href="https://www.cs.cornell.edu/jeh/book.pdf">Foundations of Data science by Avrim Blum, John Hopcroft, and Ravindran Kannan</a></li> <li><a href="https://users.cs.duke.edu/~rongge/thesis.pdf">Provable Algorithms for machine learning problems by Rong Re</a></li> <li><a href="https://web.stanford.edu/class/cs229t/2017/Lectures/percy-notes.pdf">CS229T/STAT231: Statistical Learning Theory (Winter 2016) by Percy Liang</a>https://web.stanford.edu/class/cs229t/2017/Lectures/percy-notes.pdf</li> <li><a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning: From Theory to Algorithms by Shai Shalev-Shwartz and Shai Ben-David</a></li> <li><a href="https://cs.nyu.edu/~mohri/mlbook/">Foundations of Machine Learning by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar</a></li> <li><a href="https://mitpress.mit.edu/9780262536578/">Learning with Kernels by Bernhard Schölkopf and Alexander J. Smola</a></li> <li><a href="https://jmlr.org/papers/v16/cunningham15a.html">Linear Dimensionality Reduction: Survey, Insights, and Generalizations by John P. Cunningham, Zoubin Ghahramani</a></li> <li><a href="https://arxiv.org/pdf/1803.00567.pdf">Computational optimal transport by Gabriel Peyré, Marco Cuturi</a></li> <li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning by Christopher M. Bishop</a></li> <li><a href="https://probml.github.io/pml-book/book0.html">Machine Learning: A Probabilistic Perspective by Kevin Murphy</a></li> <li><a href="https://probml.github.io/pml-book/book1.html">Probabilistic Machine Learning: An Introduction by Kevin Murphy</a></li> <li><a href="https://probml.github.io/pml-book/book2.html">Probabilistic Machine Learning: Advanced Topics by Kevin Murphy</a></li> <li><a href="https://www.gatsby.ucl.ac.uk/~porbanz/teaching/GR8201S17/slides_sampling.pdf">Sampling algorithms</a> <h2 id="papers-3">Papers</h2> </li> <li><a href="https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf">Theory of classification: A Survey of some recent advances by Stephane Boucheron, Olivier Bousquet, and Gabor Lugosi</a></li> <li><a href="https://mlg.eng.cam.ac.uk/pub/pdf/Gha12.pdf">Bayesian nonparametrics and the probabilistic approach to modelling by Zoubin Ghahramani</a></li> <li><a href="https://mlg.eng.cam.ac.uk/zoubin/papers/ijprai.pdf">An introduction to Hidden Markov Models and Bayesian networks by Zoubin Ghahramani</a></li> <li><a href="https://cs.nyu.edu/~roweis/papers/NC110201.pdf">A Unifying Review of Linear Gaussian Models by Sam Roweis, Zoubin Ghahramani</a></li> <li><a href="https://www.stats.ox.ac.uk/~teh/research/npbayes/OrbTeh2010a.pdf">Bayesian Nonparametric Models by Peter Orbanz, Yee Whye Teh</a></li> <li><a href="https://www.stats.ox.ac.uk/~teh/research/npbayes/Teh2010a.pdf">Dirichlet Process by Yee Whye Teh</a></li> <li><a href="https://www.stats.ox.ac.uk/~teh/research/npbayes/TehJor2010a.pdf">Hierarchical Bayesian Nonparametric Models with Applications by Yee Whye Teh</a> <h1 id="reinforcement-learning">Reinforcement Learning</h1> <h2 id="courses-4">Courses</h2> </li> <li><a href="https://cmudeeprl.github.io/703website_f23/">CMU 10703 - Deep RL</a></li> <li><a href="https://rail.eecs.berkeley.edu/deeprlcourse/">Berkeley CS285 - Deep RL</a></li> <li><a href="https://cs224r.stanford.edu/">Stanford CS224R - Deep RL</a></li> <li><a href="https://webdocs.cs.ualberta.ca/~machado/teaching.html">CMPUT 655 - RL 1 - Graduate</a></li> <li><a href="https://rltheory.github.io/">CMPUT 605- RL Theory Grad</a></li> <li><a href="https://courses.cs.washington.edu/courses/cse599m/19sp/#homework">Washington CSE 599 - RL and Bandits</a></li> <li><a href="https://web.mit.edu/6.7950/www/">MIT 67950 - RL Foundations and Methods</a></li> <li><a href="https://wensun.github.io/CS6789.html">Cornell CS 6789 - Foundations of RL</a></li> <li><a href="https://nanjiang.cs.illinois.edu/cs542/">Illinois CS 542 - Statistical RL</a></li> <li><a href="https://ben-eysenbach.github.io/inference-action-f23/">Princeton COS 597R - Probabilistic Topics in RL</a></li> <li><a href="https://adityam.github.io/stochastic-control/">McGill ECSE 506 - Stochastic control and decision theory</a></li> <li><a href="https://ee263.stanford.edu/">Stanford EE263: Introduction to Linear Dynamical Systems</a></li> <li><a href="https://djrusso.github.io/RLCourse/index">Columbia Dynamic Programming and RL</a></li> <li><a href="https://www.davidsilver.uk/teaching/">David Silver RL course</a></li> <li><a href="https://kamyar.page/teaching/spring_CS59300_RL_2022.html">Purdue CS58300: RL</a></li> <li><a href="https://djrusso.github.io/RLCourse/">Columbia Dynamic programming and reinforcement learning</a> <h2 id="bookssurvey-5">Books/Survey</h2> </li> <li><a href="https://arxiv.org/pdf/2301.01379.pdf">A succint Summary of Reinforcement Learning by Sanjeevan Ahilan</a></li> <li><a href="https://ics.uci.edu/~dechter/courses/ics-295/fall-2019/texts/An_Introduction_to_Deep_Reinforcement_Learning.pdf">An Introduction to Deep RL Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare and Joelle Pineau</a></li> <li><a href="">Empirical Design in Reinforcement Learning</a></li> <li><a href="https://arxiv.org/pdf/2012.13490.pdf">Towards Continual RL: A review and perspectives by Khimya Khetarpal, Matthew Rieme, Irina Rish, Doina Precup</a></li> <li><a href="https://arxiv.org/pdf/2301.08028.pdf">A survey of Meta-RL by Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, Shimon Whiteson</a></li> <li><a href="http://incompleteideas.net/book/RLbook2020.pdf">Reinforcement Learning: An Introduction by Richard S. Sutton, Andrew G. Barto</a></li> <li><a href="https://rltheorybook.github.io/rltheorybook_AJKS.pdf">Reinforcement Learning: Theory and Algorithms by Alekh Agarwal, Nan Jiang, Sham M. Kakade, Wen Sun</a></li> <li><a href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Algorithms for Reinforcement Learning by Csaba Szepesvari</a></li> <li><a href="https://www.mit.edu/~dimitrib/dpbook.html">Dynamic Programming and Optimal Control by Dimitri P. Bertsekas</a></li> <li><a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118631980">Simulation and the Monte Carlo Method by Reuven Y. Rubinstein, Dirk P. Kroese</a></li> <li><a href="https://epubs.siam.org/doi/book/10.1137/1.9780898718577">Practical Methods for Optimal Control and Estimation using nonlinear programming by John T. Betts</a></li> <li><a href="https://www.marl-book.com/">Multi-agent Reinforcement Learning: Foundations and Modern Approaches by Stefano V. Albrecht, Filippos Christianos, Lukas Schäfer</a></li> <li><a href="https://arxiv.org/pdf/2103.04047.pdf">RL, bit by bit by Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband and Zheng Wen</a></li> <li><a href="https://arxiv.org/pdf/1806.09460.pdf">A Tour of Reinforcement Learning: The View from Continuous Control by Benjamin Recht</a> <h2 id="papers-4">Papers</h2> </li> <li><a href="https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-mathematical-foundations/inverse-transform-sampling-method.html">Mathematical Foundations of Monte Carlo Methods</a></li> </ul> <h1 id="online-learning">Online Learning</h1> <h2 id="courses-5">Courses</h2> <ul> <li><a href="https://haipeng-luo.net/courses/CSCI659/2022_fall">Southern Cali CSCI 659 - Intro to Online Optimization/Learning</a></li> <li><a href="https://inst.eecs.berkeley.edu/~ee290s/fa18/">Berkeley EE290/CS194 - ML for sequential decision making under uncertainty</a></li> <li><a href="https://web.uvic.ca/~nmehta/online_learning_spring2023/">Victoria CSC 482/581 - Intro to Online Learning</a></li> <li><a href="https://courses.cs.washington.edu/courses/cse599s/14sp/">Washington CSE599 - Online Learning</a></li> <li><a href="https://people.eecs.berkeley.edu/~jiantao/2902021spring/material.html">Berkeley EE 290 - Theory of Multi-armed Bandits and RL</a></li> <li><a href="https://courses.cs.washington.edu/courses/cse599m/21sp/">Washington CSE 599M - Interactive Machine Learning in Non-stochastic Environments</a></li> <li><a href="https://yuanz.web.illinois.edu/teaching/IE498fa19/">Illinois IE 498: Online Learning and Decision Making</a></li> <li><a href="https://alekhagarwal.net/bandits_and_rl/">Columbia COMS E6998.001: Bandits and RL</a></li> <li><a href="https://www.mit.edu/~rakhlin/6.883/">MIT 6883: Online methods in ML</a></li> <li>Remark: These courses are closely related to Reinforcement Learning. <h2 id="bookssurvey-6">Books/Survey</h2> </li> <li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/BubeckLectureNotes.pdf">Introduction to Online Optimization by Sebastien Bubeck</a></li> <li><a href="https://arxiv.org/pdf/1912.13213.pdf">A modern Introduction to Online Learning by Francesco Orabona</a></li> <li><a href="https://arxiv.org/pdf/1909.05207.pdf">Introduction to Online Convex Optimization by Elad Hazan</a></li> <li><a href="https://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf">Online Learning and Online Convex Optimization by Shai Shalev-Shwartz</a></li> <li><a href="https://arxiv.org/pdf/1802.02871.pdf">Online Learning: A comprehensive Survey by Steven C.H. Hoi, Doyen Sahoo, Jing Lu, Peilin Zhao</a></li> <li><a href="https://tor-lattimore.com/downloads/book/book.pdf">Bandit algorithms by Tor Lattimore and Csaba Szepesvari</a></li> <li><a href="https://arxiv.org/pdf/1204.5721.pdf">Regret Analysis of Stochastic and Nonstochastic multi-armed bandit problem by Sebastien Bubeck and Nicolo Cesa-Bianchi</a></li> <li><a href="https://arxiv.org/pdf/1904.07272.pdf">Introduction to Multi-armed bandits by Aleksandrs Slivkins</a></li> <li><a href="https://arxiv.org/pdf/1707.02038.pdf">A tutorial on Thompson Sampling Daniel J. Russo , Benjamin Van Roy , Abbas Kazerouni, Ian Osband and Zheng Wen</a></li> <li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/ftir-online-evaluation-final-journal.pdf">Online Evaluation for Information Retrieval by Katja Hofmann, Lihong Li, Filip Radlinski</a></li> <li><a href="https://ii.uni.wroc.pl/~lukstafi/pmwiki/uploads/AGT/Prediction_Learning_and_Games.pdf">Prediction, Learning, Games by Nicolo Cesa-Bianchi, Gabor Lugosi</a></li> <li><a href="https://courses.cs.washington.edu/courses/cse599m/21sp/resources/lecture_notes.pdf">Bandit lecture notes by Kevin Jamieson</a></li> <li><a href="https://www.nowpublishers.com/article/Details/MAL-038">From Bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and planning by Remi Munos</a></li> </ul> <h2 id="papers-5">Papers</h2> <h1 id="geometric-dl-graph-nn">Geometric DL/ Graph NN</h1> <h2 id="courses-6">Courses</h2> <ul> <li><a href="https://www.youtube.com/playlist?list=PLn2-dEmQeTfSLXW8yXP4q_Ii58wFdxb3C">AMMI Geometric DL</a></li> <li><a href="https://web.stanford.edu/class/cs224w/">Stanford CS224W - Machine Learning with Graphs</a></li> <li><a href="https://gnn.seas.upenn.edu/">UPenn - GNN</a></li> <li><a href="https://web.stanford.edu/class/cs246/">Stanford CS246: Mining Massive Data Sets</a> <h2 id="bookssurvey-7">Books/Survey</h2> </li> <li><a href="https://arxiv.org/pdf/2104.13478.pdf">Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges by Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veličković</a> <h2 id="papers-6">Papers</h2> </li> </ul> <h1 id="meta-learning">Meta Learning</h1> <h2 id="courses-7">Courses</h2> <ul> <li><a href="https://cs330.stanford.edu/">Stanford CS 330 - Deep Multi-task and meta learning</a> <h2 id="bookssurvey-8">Books/Survey</h2> <h2 id="papers-7">Papers</h2> <h1 id="representation-learning">Representation Learning</h1> <h2 id="courses-8">Courses</h2> </li> <li><a href="https://sites.google.com/mila.quebec/ift6135">Mila IFT 6135 Representation Learning</a> <h2 id="bookssurvey-9">Books/Survey</h2> <h2 id="papers-8">Papers</h2> <h1 id="pgm">PGM</h1> <h2 id="courses-9">Courses</h2> </li> <li><a href="https://ermongroup.github.io/cs228/">Stanford CS228 - PGM</a></li> <li><a href="https://erdogdu.github.io/csc412/">UofT CSC 412 - Probabilistic ML</a></li> <li><a href="https://andrejristeski.github.io/10708-F22/schedule.html">CMU 10708 - PGM</a> <h2 id="bookssurvey-10">Books/Survey</h2> </li> <li>[Probabilistic graphical model: Principles and techniques by Daphne Koller and Nir Friedman] <h2 id="papers-9">Papers</h2> <h1 id="statistics">Statistics</h1> <h2 id="courses-10">Courses</h2> </li> <li><a href="https://www.cs.toronto.edu/~cmaddis/courses/sta314_f21/">UofT STA314H1F - Statistical Learning theory I</a></li> <li><a href="https://duvenaud.github.io/sta414/">Uoft STA414 - Statistical Learning theory II</a></li> <li><a href="https://inst.eecs.berkeley.edu/~ee126/sp23/">Berkeley EECS 126 - Probability and Random processes</a></li> <li><a href="https://people.eecs.berkeley.edu/~jordan/courses/210B-spring17/">Berkeley Stat210B - Theoretical Statistics</a></li> <li><a href="https://www.stat.cmu.edu/~larry/=stat705/">CMU 36705 - Intermediate Statistics</a></li> <li><a href="https://web.stanford.edu/class/stats300b/">Stanford STATS 300B: Theory of Statistics II</a></li> <li><a href="https://web.stanford.edu/class/stats311/">Statistics 311/Electrical Engineering 377: Information Theory and Statistics</a></li> <li><a href="https://web.stanford.edu/class/ee378b/ee378b.html">Stanford EE 378B – Inference, Estimation, and Information Processing</a></li> <li><a href="https://people.seas.harvard.edu/~madhusudan/courses/Spring2016/">Harvard CS 229r - Information Theory in Computer Science</a></li> <li><a href="https://people.seas.harvard.edu/~madhusudan/courses/Spring2017/">Harvard CS 229r - Essential Coding Theory</a></li> <li><a href="https://learning-modules.mit.edu/materials/index.html?uuid=/course/18/sp18/18.657#materials">MIT 18657 - High dimensional probability</a></li> <li><a href="https://web.math.princeton.edu/~asly/GradProb.html">Princeton MAT 589 - Topics in Probability, Statistics and Dynamics: Modern discrete probability theory</a> <h2 id="bookssurvey-11">Books/Survey</h2> </li> <li><a href="https://cs229.stanford.edu/section/cs229-prob.pdf">Probability Theory Survey by Arian Maleki and Tom Do</a></li> <li><a href="https://statproofbook.github.io/">Statistics Proof book</a></li> <li><a href="https://www.cambridge.org/highereducation/books/probability-with-martingales/B4CFCE0D08930FB46C6E93E775503926#overview">Probability with Martingales by David Williams</a></li> <li><a href="https://www.cambridge.org/core/search?filters%5BauthorTerms%5D=Martin%20J.%20Wainwright&amp;eventCode=SE-AU">High-dimensional statistics: A non-asymptotic viewpoint by Martin J. Wainwright</a></li> <li><a href="https://book-wright-ma.github.io/Book-WM-20210422.pdf">High-dimensional data analysis with low-dimensional models: principles, computation, and applications by John Wright and Yi Ma</a></li> <li><a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html">High-dimensional Probability: An introduction with Applications in Data Science by Roman Vershynin</a></li> <li><a href="https://web.math.princeton.edu/~rvan/APC550.pdf">Probability in High dimension by Ramon van Handel (Princeton)</a></li> <li><a href="http://www.econ.upf.edu/~lugosi/mlss_slt.pdf">Introduction to Statistical Learning Theory Olivier Bousquet, Stephane Boucheron, and Gabor Lugosi</a></li> <li><a href="https://maxim.ece.illinois.edu/teaching/SLT/SLT.pdf">Illinois ECE 543 Statistical Learning Theory by Bruce Hajek and Maxim Raginsky</a></li> <li><a href="https://www.stats.ox.ac.uk/~deligian/pdf/statml/notes_ver2.pdf">Oxford Modern Statistical Theory by George Deligiannidis</a></li> <li><a href="https://www.stat.cmu.edu/~larry/all-of-statistics/">All of statistics: A concise Course in statistical inference by Larry Wasserman</a></li> <li><a href="https://www.stat.cmu.edu/~larry/all-of-nonpar/index.html">All of nonparametric statistics by Larry Wasserman</a></li> <li><a href="https://link.springer.com/book/10.1007/b13794">Introduction to Nonparametric Estimation by Alexandre B. Tsybakov</a></li> <li><a href="https://link.springer.com/book/10.1007/b97553">Mathematical Statistics by Jun Shao</a></li> <li><a href="https://www.cambridge.org/core/books/asymptotic-statistics/A3C7DAD3F7E66A1FA60E9C8FE132EE1D">Asymptotic Statistics by A. W. van der Vaart</a></li> <li><a href="https://www.semanticscholar.org/paper/A-Survey-on-Distribution-Testing%3A-Your-Data-is-Big.-Canonne/6d6d1d090760a51e9eac5f71cb0b71eb3aaab680?p2df">A Survey on Distribution Testing: Your Data is Big. But is it Blue? by C. Canonne</a></li> <li><a href="https://arxiv.org/pdf/1011.3027.pdf">Introduction to the non-asymptotic analysis of random matrices by Roman Vershynin</a></li> <li><a href="https://www.hse.ru/data/2016/11/24/1113029206/Concentration%20inequalities.pdf">Concentration inequalities: A nonasymptotic theory of independence by Stephane Boucheron, Gabor Lugosi, Pascal Massart</a></li> <li><a href="http://users.cms.caltech.edu/~jtropp/books/Tro14-Introduction-Matrix-FnTML-rev.pdf">An Introduction to Matrix Concentration Inequalities by Joel A. Tropp</a></li> <li><a href="https://people.math.wisc.edu/~roch/mdp/index.html">Modern Discrete Probability: An Essential Toolkit by Sebastien Roch</a></li> <li><a href="http://www.stevenheilman.org/~heilman/teach/170asn.pdf">MATH 170A - Probability theory by Steven Heilman</a></li> <li><a href="http://www.stevenheilman.org/~heilman/teach/170bf.pdf">MATH 170B - Probability theory by Steven Heilman</a></li> <li><a href="https://hastie.su.domains/Papers/ESLII.pdf">The elements of statistical learning: Data mining, inference and prediction - Trevor Hastie, Robert Tibshirani, Jerome Friedman</a></li> <li><a href="https://www.seyedkalali.com/wp-content/uploads/2016/11/A-First-Course-in-Probability-8th-ed.-Sheldon-Ross.pdf">A first course in probability by Sheldon Ross</a></li> <li><a href="https://people.bu.edu/pekoz/A_Second_Course_in_Probability-Ross-Pekoz.pdf">A second course in probability by Sheldon Ross</a></li> <li><a href="https://www.amazon.com/Mathematical-Statistics-Applications-Dennis-Wackerly/dp/0495110817">Mathematical Statistics with applications by Wackerly, Mendenhall, Scheaffer</a></li> <li><a href="https://minerva.it.manchester.ac.uk/~saralees/statbook2.pdf">Introduction to mathematical statistics by Hogg, McKean, Craig</a></li> <li><a href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X">Elements of information theory by Thomas Cover, Joy Thomas</a></li> <li><a href="https://users.stat.ufl.edu/~winner/sta4211/ALSM_5Ed_Kutner.pdf">Applied linear statistical models by Michael Kutner, Christopher Nachtsheim, John Neter, William Li</a></li> <li><a href="https://www.jstor.org/stable/4355554?seq=1#page_scan_tab_contents">Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory by Lawrence D. Brown</a> <h2 id="papers-10">Papers</h2> </li> </ul> <h1 id="nlp">NLP</h1> <h2 id="courses-11">Courses</h2> <ul> <li><a href="https://cs224d.stanford.edu/">Stanford CS224d - Deep Learning for Natural Language Processing</a></li> <li><a href="https://harvard-ml-courses.github.io/cs287-web/">Harvard CS287 - Machine Learning for Natural Language</a></li> <li><a href="https://phontron.com/class/nn4nlp2018/">CMU CS 11-747 - Neural Networks for NLP </a></li> <li><a href="https://phontron.com/class/mtandseq2seq2018/">CMU CS 11-731 - Machine Translation and Sequence-to-sequence Models</a></li> <li><a href="https://www.cs.princeton.edu/courses/archive/spring18/cos495/schedule/">Princeton COS495 Natural Language Processing</a> <h2 id="surveysbooks">Surveys/Books</h2> </li> <li><a href="https://u.cs.biu.ac.il/~yogo/nnlp.pdf">A Primer on Neural Network Models for Natural Language Processing by Yoav Goldberg</a> <h2 id="papers-11">Papers</h2> </li> </ul> <h1 id="foundations">Foundations</h1> <h2 id="courses-12">Courses</h2> <ul> <li><a href="https://www.cs.cmu.edu/~15451-s20/schedule.html">CMU 15451 - Algorithms</a></li> <li><a href="https://www.cs.cmu.edu/~213/schedule.html">CMU 15213 - Introduction to computer systems</a></li> <li><a href="https://www.cs.cmu.edu/~213/index.html">CMU 15-213/15-513/14-513 - Introduction to Computer Systems</a></li> <li><a href="https://web.stanford.edu/class/archive/cs/cs111/cs111.1234/calendar">Stanford CS 111 - OS</a></li> <li><a href="https://people.seas.harvard.edu/~cs125/">Harvard CS 125: Algorithms and Complexity</a></li> <li><a href="https://timroughgarden.github.io/fob21/">Columbia COMS 6998-006 - Foundations of Blockchains</a></li> <li><a href="https://timroughgarden.org/f19/f19.html">Columbia COMS 4995 - Randomized Algorithms</a> <h2 id="bookssurvey-12">Books/Survey</h2> </li> <li><a href="http://cs-www.cs.yale.edu/homes/aspnes/classes/469/notes.pdf">Notes on Randomized Algorithms by James Aspnes</a></li> <li> <h2 id="papers-12">Papers</h2> </li> </ul>]]></content><author><name></name></author><category term="resources"/><category term="resources"/><category term="ml"/><category term="rl"/><summary type="html"><![CDATA[Resources related to ML, RL, Math, Stat, CS, etc.]]></summary></entry><entry><title type="html">Reinforcement Learning</title><link href="https://chuducthang77.github.io/blog/2023/rl/" rel="alternate" type="text/html" title="Reinforcement Learning"/><published>2023-02-03T00:00:00+00:00</published><updated>2023-02-03T00:00:00+00:00</updated><id>https://chuducthang77.github.io/blog/2023/rl</id><content type="html" xml:base="https://chuducthang77.github.io/blog/2023/rl/"><![CDATA[<p>This page is in the process of editing. Some details and references are still missing. Please forgive me!!</p> <h2 id="fundamental-concept-of-rl">Fundamental concept of RL</h2> <p>This section includes notations (different versions) as well as important formulas. Derivations and specific details will be discussed in later sections.</p> <ul> <li><strong>Remark</strong>: Please note that any characters with subscript t or capital letter will be random variables</li> <li><strong>Reinforcement Learning vs Optimal Control</strong></li> <li><strong>Agent</strong>: receives $o_t$ and $r_t$, emit $a_t$</li> <li><strong>Environment</strong>: Agent lives and interact with, receives $a_t$, emits $o_t$ and $r_{t+1}$</li> <li><strong>Markov property</strong>: $P(s_{t+1} | s_1, a_1, …, s_t, a_t) = P(s_{t+1} |s_t, a_t)$</li> <li><strong>Reward</strong>: $r_t=R(s_t, a_t, s_{t+1})$ or $R(s_t,a_t)$ or $R(s_t)$ <ul> <li><strong>Definition</strong>: Scalar signal received from the environment, estimation of the current state</li> <li><strong>Matrix format</strong>: $\textbf r\in\mathbb R^{|\mathcal S||\mathcal A| \times 1}$</li> <li><strong>Deterministic</strong> vs <strong>stochastic</strong> (a random variable with zero mean and non-zero variance)</li> <li><strong>Bounded reward</strong>: $[0, R_{max}]$</li> <li><strong>Remark</strong>: agent does not control the reward it receives or randomness of the env</li> <li><strong>Reward hypothesis</strong>: All goals can be described by the maximization of expected cumulative reward</li> <li><strong>Reward function</strong>: $R(s_t = s, a_t = a) = \mathbb E[R_t | s_t= s, a_t = a]= \sum_{r\in \mathbb R}\sum_{s'\in S} r P(s',r | s,a)$ <ul> <li>$R(s_t = s, a_t = a, s_{t+1} = s') = \mathbb E[R_t | s_t= s, a_t = a, s_{t+1} = s'] = \sum_{t=0} \sum_{r\in\mathbb R} \gamma^t r \frac{P(s', r|s, a)}{P(s'|s, a)}$</li> </ul> </li> </ul> </li> <li><strong>Return</strong>: $G_t$ or $R(\tau)$ <ul> <li><strong>Definition</strong>: cumulative reward, goal of the agent (maximize)</li> <li><strong>Finite-horizon undiscounted return</strong>: $R(\tau) = \sum_{t=0}^T r_t = \sum_{t=0}^T R(s_t, a_t) $</li> <li><strong>Infinite-horizon discounted return</strong>: $R(\tau) = \sum_{t=0}^{\infty} \gamma^tr_t = \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)$ (mathematically convenient, but not implement in practice)</li> <li><strong>Expected Return</strong>: <ul> <li><strong>Continuous state action space</strong>:$J(\pi) = \mathbb E_{\tau\sim\pi,P,\rho_0}[R(\tau)] = \int_{\tau} P(\tau|\pi)R(\tau) = \int_{\tau} \rho_0(s_0)\prod_{t=0}^{T-1}P(s_{t+1}|s_t,a_t)\pi(a_t|s_t)R(\tau)$</li> <li><strong>Discrete state action space</strong>: $J(\pi) = \mathbb E_{\tau\sim\pi,P,\rho_0}[R(\tau)] = \sum_{s} \sum_{a} P(s_t = s|\rho_0, \pi, P)\pi(a_t = a|s_t= s) r(s_t = s,a_t = a)$</li> </ul> </li> </ul> </li> <li><strong>State</strong>: $s_t$ <ul> <li><strong>Definition</strong>: Complete description of the state, no hidden information from the world, Markov property</li> <li><strong>State transition</strong>: deterministic ($f(s_t,a_t)$) vs stochastic ($s_{t+1} \sim P(.|s_t,a_t)$)</li> </ul> </li> <li><strong>History</strong>: $\mathcal H_t = a_1, o_1, r_1,…, a_t, o_t, r_t$, $f(\mathcal H_t) = s_t^\alpha \approx s_t$</li> <li><strong>Observation</strong>: $o_t$, partial description of a state, generally not Markov property</li> <li><strong>Observability</strong>: $s_t \equiv o_t$</li> <li><strong>Action</strong>: $a_t$, discrete (left/right) vs continuous (speeding)</li> <li><strong>Trajectories</strong>: $\tau = (s_0, a_0, s_1, a_1, …)$, episodes or rollouts</li> <li><strong>Task</strong>: Continuing (infinite T) and discrete (finite T)</li> <li><strong>Markov processes</strong>: $\langle \mathcal S, P \rangle$ <ul> <li>$P: \mathcal S \rightarrow \Delta(\mathcal S)$</li> </ul> </li> <li><strong>Markov reward processes</strong>: $\langle \mathcal S, P, R, \gamma\rangle$ <ul> <li>$R: \mathcal S \rightarrow \mathbb{R}$</li> <li>$P: \mathcal S \rightarrow \Delta(\mathcal S)$</li> </ul> </li> <li><strong>Markov decision processes</strong>: $\langle \mathcal S, \mathcal A, R, P,\gamma, \rho_0\rangle$ <ul> <li>$R: \mathcal S\times \mathcal A\times \mathcal S \rightarrow \mathbb{R}$</li> <li>$P:\mathcal S\times\mathcal A\rightarrow \Delta(\mathcal S)$</li> </ul> </li> <li><strong>Partially Observable Markov Decision Process</strong>: $\langle \mathcal S, \mathcal A, \mathcal O, P, R, \mathcal Z, \gamma, \rho_0\rangle$ <ul> <li>$\mathcal O$: Finite set of observations</li> <li>$\mathcal Z$: Observation function</li> <li>Belief state $b(h) = (P(s_t = s_1 | H_t = h), … , P(s_t = s_{|\mathcal S |}) | H_t = h)$</li> </ul> </li> <li><strong>Probability transition</strong>: <ul> <li><strong>Four-argument dynamics</strong>: $P(s', r | s, a) = \Pr(s_{t+1} = s', R_t = r | s_t = s, a_t =a )$</li> <li><strong>Three-argument dynamics</strong>: $P(s' | s, a) = \sum_{r} \Pr(s_{t+1} = s', R_t = r | s_t = s, a_t =a )$</li> <li><strong>Matrix format</strong>: $\textbf P \in\mathbb [0,1]^{|\mathcal S||\mathcal A|\times | \mathcal S|}$</li> </ul> </li> <li><strong>Policy</strong>: $\pi$ <ul> <li><strong>Definition</strong>: Rules used by agents</li> <li><strong>Matrix format</strong>: $\Pi \in [0,1]^{| \mathcal S |\times | \mathcal S| | \mathcal A |}$ = $diag(\pi(a|s_1) \pi(a|s_2) … \pi(a|s_{| \mathcal S|}))$</li> <li><strong>Non-stationary</strong> :$\mathcal H \rightarrow \mathcal A$</li> <li><strong>Stationary</strong>: $\mathcal S \rightarrow \Delta(\mathcal A)$ or $\mathcal S \rightarrow \mathcal A$ <ul> <li><strong>Deterministic</strong>: $a=\mu(s)$ or $a=\mu_\theta(s)$, MLP</li> <li><strong>Stochastic</strong>: $a\sim\pi(.|s)$ or $a\sim\pi_\theta(.|s)$, sample from $\pi(.|s)$ and compute $\log\pi(a|s)$ <ul> <li><strong>Categorical</strong>: discrete, MLP + softmax, argmax to choose the function</li> <li><strong>Diagonal Gaussian</strong>: continuous, reparameterization trick, $a = \mu_\theta(s) + \sigma_\theta(s) \odot z$ with $z\sim \mathbb{N}(0,I)$</li> </ul> </li> </ul> </li> <li><strong>Optimal policy</strong>: $\pi^* = \arg\max_\pi J(\pi)$, multiple optimal policies, but same value function</li> <li><strong>Better policy</strong>: $\pi' \ge \pi$ iff $V^\pi(s) \ge V^{\pi'}(s)\quad\forall s\in \mathcal S$</li> <li><strong>Greedy policy</strong>: $\pi_Q(s) = \arg\max_a Q(s,a)$</li> <li>$\epsilon$<strong>-optimal policy</strong>: $V^\pi(s) \ge V^*(s) - \epsilon\textbf 1$</li> </ul> </li> <li><strong>Values</strong> <ul> <li><strong>(On-policy) Value function</strong>: $V^{\pi}(s) = \mathbb E_{\tau \sim \rho_0, P, \pi}[R(\tau)|s_0=s] $, starts at s, then follow $\pi$ <ul> <li><strong>Definition</strong>: An estimation of how good it is for an agent to be in a state</li> <li><strong>Matrix format</strong>: $\textbf V^\pi \in \mathbb R^{|\mathcal S|}$</li> </ul> </li> <li><strong>(On-policy) Action-value function</strong>: $Q^{\pi}(s,a) = \mathbb E_{\tau\sim\rho_0, P, \pi}[R(\tau)|s_0=s, a_0=a]$, starts at s, random action a (not follow $\pi$), follow $\pi$ <ul> <li><strong>Definition</strong>: An estimation of how good it is for an agent to be in a state and taking an action</li> <li><strong>Matrix format</strong>: $\textbf Q^\pi \in \mathbb R^{|\mathcal S||\mathcal A|}$</li> </ul> </li> <li><strong>Optimal value function</strong>: $V^* (s) = \max_\pi V^{\pi}(s) = \max_a Q^*(s,a)$, starts at s, then follow $\pi^{*}$</li> <li><strong>Optimal action-value function</strong>: $Q^*(s,a) = \max_\pi Q^{\pi}(s,a)$, starts at s, random action a (not follow $\pi^{*}$), then follow $\pi^{*}$</li> <li><strong>Optimal action</strong>: $a^* = \pi^{*}(a| s) = \arg\max_a Q^* (s,a)$, maybe multiple optimal actions</li> </ul> </li> <li><strong>Bellman Equations</strong>: <ul> <li><strong>(On-policy) Value function</strong>: $V^{\pi}(s) = \mathbb E_{a\sim \pi(.|s),s'\sim P(.|s, a)} [r(s,a,s') + \gamma V^{\pi}(s')] = \mathbb E_{a\sim \pi(.|s)} [ Q^\pi(s,a) ] = \sum_{a}Q^\pi(s,a)\pi(a|s)$ <ul> <li><strong>Matrix format</strong>: $\textbf V^\pi = \Pi\textbf r + \gamma\Pi\textbf P\textbf V^\pi $</li> </ul> </li> <li><strong>(On-policy) Action-value function</strong>: $Q^\pi(s,a) = \mathbb E_{s'\sim P(.|s, a)}[r(s,a,s') + \gamma \mathbb E_{a'\sim \pi(.|s)}[Q^\pi(s',a')]] = \mathbb E_{s'\sim P(.|s, a)}[r(s,a,s') + \gamma V^\pi(s')]]$ <ul> <li><strong>Matrix format</strong>: $\textbf Q^\pi = \textbf r + \gamma \textbf P \Pi \textbf Q^\pi = \textbf r + \gamma \textbf P \textbf V^\pi $ <ul> <li><strong>Corollary</strong>: The matrix $\textbf I - \gamma \textbf P \Pi$ is invertible</li> </ul> </li> <li><strong>Remark</strong>: Bootstrapping update</li> </ul> </li> <li><strong>Optimal value function</strong>: $V^* (s) = \max_a \mathbb E_{s' \sim P(.|s, a)}[ r(s,a,s') + \gamma V^* (s')]$</li> <li><strong>Optimal action-value function</strong>: $Q^* (s,a) = \mathbb E[r_{t+1} + \gamma V^\pi(s_{t+1}) | s_t = s, a_t = a ]=\mathbb E_{s' \sim P(.|s, a)}[r(s,a,s') + \gamma \max_{a'} Q^* (s', a')]$ <ul> <li><strong>Remark</strong>: Do not care about the transition tuples, how actions are selected, because it should satisfy Bellman equation for all possible transitions</li> </ul> </li> </ul> </li> <li><strong>Occupancy measure</strong>: stationary distribution over $\mathcal S \times \mathcal A$ or $\mathcal A$ space, induced by running policy $\pi$ in the environment. <ul> <li><strong>State-action</strong>: <ul> <li><strong>Unnormalized version</strong>: $\hat \rho^\pi(s,a) = \sum_{t=0}^\infty \gamma^t P(s_t = s, a_t = a| \rho_0, \pi, P)$ <ul> <li><strong>Matrix format</strong>: $(\textbf I - \gamma \textbf P \Pi)^{-1} \in [0,1]^{|\mathcal S | |\mathcal A | \times |\mathcal S | |\mathcal A |}$</li> </ul> </li> <li><strong>Normalized version</strong>: $\rho^\pi(s,a) = \frac{\hat \rho^\pi(s,a)}{\sum_{s, a}\hat \rho^\pi(s,a)} = (1-\gamma) \hat \rho^\pi (s,a)$</li> </ul> </li> <li><strong>State</strong>: $\rho(s)$ <ul> <li><strong>Unnormalized version</strong>: $\hat \rho^\pi(s) = \sum_{t=0}^\infty \gamma^t P(s_t = s| \rho_0, \pi, P)$ <ul> <li><strong>Matrix format</strong>: $(\textbf I - \gamma \Pi \textbf P)^{-1} \in [0,1]^{|\mathcal S | \times |\mathcal S |}$</li> </ul> </li> <li><strong>Normalized version</strong>: $\rho^\pi(s) = \frac{\hat \rho^\pi(s)}{\sum_{s}\hat \rho^\pi(s)} = (1-\gamma) \hat \rho^\pi (s)$</li> </ul> </li> <li><strong>Lemma</strong>: 2 policies $\pi$ and $\pi'$ are equivalent iff they have the same $V^\pi(s) = V^{\pi'}(s) \forall s \in \mathcal S$ or $\rho^\pi(s,a) = \rho^{\pi'}(s,a), \forall s\in\mathcal S,a\in\mathcal A$</li> <li><strong>Lemma</strong>: For all $\pi$, there always exists a stationary policy $\pi'$ with the same occupancy measure. <ul> <li><strong>Corollary</strong>: It is sufficed to consider stationary policy.</li> </ul> </li> <li><strong>Expected return</strong>:</li> <li><strong>Value function</strong>:</li> <li><strong>Action-value function</strong>:</li> </ul> </li> <li><strong>Advantage functions</strong>: $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$ <ul> <li><strong>Definition</strong>: How much an action is better than others</li> <li><strong>Remark</strong>: It is easier to learn the consequence of an action is better than the other, than it is to learn the actual return from taking actions</li> </ul> </li> <li><strong>Models</strong> <ul> <li><strong>Model-based</strong>: learn $P(.|s_t. a_t)$ and $r$, sample efficiency, planning, sometimes no ground-truth model, bias leads to suboptimal performance, still challenge in model-learning <ul> <li><strong>Pure planning</strong>: model predictive control to select actions, not policy, MBMF</li> <li><strong>Expert iteration</strong>: learn explicit representation of $\pi_\theta(a|s)$, planning (MCTS) and generate actions, AlphaZero</li> <li><strong>Data augmentation</strong>: use fictitious and/or real experience to learn $\pi(.|s)$ or $Q^\pi$, MBVE, World Models</li> <li><strong>Embedding planning loops</strong>: embed planning into policy as a subrountine, l2A</li> </ul> </li> <li><strong>Model-free</strong>: Easy to implement, tune, not sample efficiency <ul> <li><strong>Policy optimization</strong>: learn approximator $\pi_\theta(a|s)$ for $\pi(a|s)$, directly or indirectly optimize $J(\pi_\theta)$, on-policy (most updated version of policy), usually involves learning $V_\theta(s)$ for $V^\pi(s)$, A2C, A3C, PPO</li> <li><strong>Q-Learning</strong>: learn approximator $Q_\theta(s,a)$ for $Q^*$, off-policy (data collected at any point during training), DQN, C51</li> <li><strong>Trade-off</strong>: direct vs indirect optimization, stable vs sample efficiency.</li> <li><strong>Interpolation</strong>: under some circumstances $\rightarrow$ equivalent, DDPG, SAC</li> </ul> </li> </ul> </li> <li><strong>Prediction vs Control</strong> <ul> <li><strong>Prediction</strong>: <ul> <li><strong>Input</strong>: MDP or MRP and $\pi$</li> <li><strong>Output</strong>: $V^{\pi}$ or $Q^{\pi}$</li> </ul> </li> <li><strong>Control</strong>: <ul> <li><strong>Input</strong>: MDP or MRP</li> <li><strong>Output</strong>: $V^{*}$ or $Q^{*}$</li> </ul> </li> </ul> </li> <li><strong>Bandit</strong></li> <li><strong>Dynamic Programming</strong>: Optimal substructure + Overlapping subproblems <ul> <li><strong>Algorithm</strong>: Policy iteration, value iteration</li> <li><strong>Synchronous vs Asynchronous</strong> (in-place, prioritised sweeping, real-time)</li> <li><strong>Properties</strong>: full-width backups, effective for medium-sized problems (millions of states)</li> <li><strong>Drawback</strong>: Computationally expensive and known MDP</li> </ul> </li> <li><strong>Policy iteration</strong>: Policy evaluation + Policy improvement <ul> <li><strong>Policy evaluation</strong>: $V^{k+1}(s) = \mathbb E_{a\sim \pi(.|s),s'\sim P(.|s, a)} [r(s,a,s') + \gamma V^{k}(s')]$ (iterative) <ul> <li><strong>Matrix format</strong>: $\textbf V^{k+1} = \Pi\textbf r + \gamma\Pi\textbf P\textbf V^{k} $</li> </ul> </li> <li><strong>Policy improvement</strong>: $\pi’= $greedy($V^{\pi}$) = $\arg\max_a Q^\pi(s|a)$ (greedy)</li> <li>Always converge to $\pi^*$</li> </ul> </li> <li><strong>Principle of Optimality</strong>: The policy $\pi$ can achieve optimal value from state s $V^*(s)$ if and only if any state $s'$ is reachable from s and $\pi$ achieve optimal value from state $s'$</li> <li><strong>Value iteration</strong>: inspired by principle of optimality <ul> <li>$V^* (s) = \max_a \mathbb E_{s' \sim P(.|s, a)}[ r(s,a,s') + \gamma V^* (s')]$</li> <li><strong>Remark</strong>: No explicit policy like policy iteration</li> </ul> </li> <li><strong>Policy optimization</strong> <ul> <li><strong>Assumption</strong>: Finite-horizon undiscounted return</li> <li><strong>Policy gradient algorithms</strong>: Derive analytical gradient $\rightarrow$ Form a sample estimate of expected return</li> <li><strong>Formula</strong>: $\theta_{k+1} = \theta_{k} + \alpha \nabla_\theta J(\pi_\theta)|_{\theta_k}$</li> </ul> </li> </ul> <p>\(\begin{aligned}\nabla_\theta J(\pi_\theta) &amp;= \nabla_\theta \mathbb E_{\tau\sim\pi_\theta}[R(\tau)] \\ &amp;= \nabla_\theta \int_\tau P(\tau|\pi_\theta)R(\tau)\\ &amp;= \int_\tau \nabla_\theta P(\tau|\pi_\theta)R(\tau)\\ &amp;= \int_\tau P(\tau|\pi_\theta)\nabla_\theta \log P(\tau|\pi_\theta)R(\tau) &amp; \text{Log derivative trick}\\ &amp;= \int_\tau P(\tau|\pi_\theta)\sum_{t=0}^T\nabla_\theta \log \pi_\theta(a_t|s_t)R(\tau) &amp; \text{Only }\pi\text{ depends on } \theta\\ &amp;= \mathbb E_{\tau\sim\pi_\theta}[\sum_{t=0}^T\nabla_\theta \log \pi_\theta(a_t|s_t)R(\tau) ]\\ &amp;= \mathbb E_{\tau\sim\pi_\theta}[\sum_{t=0}^T\nabla_\theta \log \pi_\theta(a_t|s_t)\sum_{t'=t}^T R(s_{t'}, a_{t'}, a_{t'+1}) ] &amp; \text{Reward-to-go}\\ &amp;= \mathbb E_{\tau\sim\pi_\theta}[\sum_{t=0}^T\nabla_\theta \log \pi_\theta(a_t|s_t)(\sum_{t'=t}^T R(s_{t'}, a_{t'}, a_{t'+1}) - b(s_t)) ] &amp; \text{Baseline in PG}\\ &amp;= \mathbb E_{\tau\sim\pi_\theta}[\sum_{t=0}^T\nabla_\theta \log \pi_\theta(a_t|s_t)\Phi_t ] &amp; \text{Baseline in PG} \end{aligned}\)</p> <ul> <li><strong>Reward-to-go</strong>: Previous rewards does not influence the current decision, Reduces variance, improves sample efficiency.</li> <li><strong>Expected Grad-Log-Prob Lemma:</strong> $\mathbb E_{\tau\sim\pi_\theta}[\nabla_\theta \log \pi_\theta(a_t|s_t)] = 0$</li> <li><strong>Baseline in Policy Gradient</strong>: Inspired by Expected Grad-Log-Prob Lemma, $b(s_t)$ does not depend on $\theta$ ($V^{\pi}(s_t)$), reduces variance.</li> <li><strong>Remark</strong>: $\Phi_t$ can be full reward, reward-to-go, reward-to-go minus the baseline, $Q^{\pi_\theta}(s_t,a_t)$, $A^{\pi_\theta}(s_t,a_t)$</li> <li><strong>Remark</strong>: Data distribution depends on the generated policy, which depends on $\theta$</li> <li><strong>Remark</strong>: Loss function performing well does not mean improving expected return. Only care about average return, not loss.</li> <li><strong>Remark</strong>: $V^{\pi}(s_t)$ is approximated by NN $V_\phi(s_t)$, and updated concurrently with the policy <ul> <li><strong>Objective</strong>: $\phi_k = argmin_{\phi} \mathbb E_{s_t, \hat R_t \sim\pi_k}[(V_\phi(s_t) - \hat R_t)^2 ]$ (epoch k)</li> </ul> </li> </ul> <h2 id="deep-rl">Deep RL</h2> <ul> <li><strong>Key factors</strong>: reliability, sample efficiency</li> <li><strong>On-policy</strong>: <ul> <li>VPG $\rightarrow$ TRPO $\rightarrow$ PPO</li> <li>Trade-off sample efficiency (not use old data) and reliability (optimize directly policy performance)</li> </ul> </li> <li><strong>Off-policy</strong>: <ul> <li>DDPG $\rightarrow$ TD3 $\rightarrow$ SAC</li> <li>Exploit Bellman’s equations $\rightarrow$ satisfy any environment interaction data</li> <li>No guarantees to have better policy performance.</li> </ul> </li> <li><strong>VPG</strong>: on-policy algorithm, not much exploration, use in either discrete and continuous action space <ul> <li><strong>Objective (To update $\theta$, then $\phi$)</strong>:</li> </ul> </li> </ul> \[\theta_{k+1} = \theta_k + \alpha_k\frac{1}{|D|}\sum_{\tau\in D}\sum_{t=0}^T\nabla_{\theta} \log \pi_\theta(a_t|s_t)\|_{\theta_k}\hat A_t\] \[\phi_{k+1} = \arg\min_\phi \frac{1}{|D|T}\sum_{\tau\in D}\sum_{t=0}^T (V_\phi(s_t) - \hat R_t)^2\] <ul> <li><strong>TRPO</strong>: on-policy algorithm, the largest possible step that satisfy the constraint on how close new and old policies using KL divergence. <ul> <li><strong>Objective</strong>:</li> <li> \[\begin{aligned} \theta_{k+1} &amp;= \arg\max_\theta L(\theta_k, \theta) &amp; \text{ s.t }D_{KL}(\theta||\theta_k) \le \delta\\ &amp;= \arg\max_\theta \mathbb E_{s,a\sim\theta_{\theta_k}}[\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)] &amp; \text{ s.t }E_{s\sim\pi_{\theta_k}}[D_{KL}(\pi_\theta(.|s)||\pi_{\theta_k}(.|s))] \le \delta\\ &amp;= \arg\max_\theta g^T(\theta-\theta_k) &amp; \text{ s.t } \frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) \le \delta \\ &amp;= \theta_k + \sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g &amp; \text{Lagrangian duality}\\ &amp;= \theta_k + \alpha^j\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g &amp; \text{Backtracking}\\ &amp;= \theta_k + \alpha^j\sqrt{\frac{2\delta}{g^T\hat{x}}}\hat{x} &amp; \text{Expensive computation } H^{-1} \end{aligned}\] </li> </ul> </li> <li><strong>PPO</strong>: Similar idea to TRPO, but relaxed objective. The distance between old policy and new policy is constrained by $1+\epsilon$ \(\theta_{k+1} = \arg\max_\theta \mathbb E_{s,a \sim\pi_{\theta_k}}[L(s,a,\theta_k,\theta)]\)</li> </ul> \[\begin{aligned} L(s,a,\theta_k,\theta) &amp; = \min (\frac{\pi_\theta(a|s)}{\pi_{\theta_k}}(a|s)A^{\pi_{\theta_k}(s,a)}, clip(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, 1-\epsilon, 1+\epsilon)A^{\pi_{\theta_k}}(s,a)) \\ &amp;= \min (\frac{\pi_\theta(a|s)}{\pi_{\theta_k}}(a|s)A^{\pi_{\theta_k}(s,a)}, g(\epsilon, A^{\pi_{\theta_k}}(s,a)) \end{aligned}\] <ul> <li><strong>DDPG</strong>: off-policy (replay buffer contains data generated by outdated policy), continuous action spaces, concurrently learns Q-function (Bellman equation and off-policy data) and $\pi$ (Q-function) <ul> <li><strong>Assumption</strong>: $Q^*(s,a)$ is differentiable with respect to a</li> <li><strong>Goal</strong>: Learns $Q_\phi(s,a)$ to approximate $Q^*(s,a) = \mathbb E_{s'\sim P} [ r(s,a) + \gamma max_{a'} Q^{*} (s',a')]$</li> <li><strong>Objective</strong>:</li> </ul> </li> </ul> \[\begin{aligned}L(\phi,D) &amp;= \mathbb E_{s,a,r,s',d\sim D}[(Q_\phi(s,a) - (r + \gamma(1-d)Q_{\phi_{targ}}(s',\mu_{\theta_{targ}}(s'))))^2] \\ &amp;\approx \frac{1}{B}\sum_{(s,a,r,s',d) \in B} (Q_\phi(s,a) - (r + \gamma(1-d)Q_{\phi_{targ}}(s',\mu_{\theta_{targ}}(s'))))^2 \end{aligned}\] \[E_{s\in D}[Q_\phi(s,\mu_\theta(s))] \approx \frac{1}{B} \sum_{s\in B}Q_\phi(s,\mu_\theta(s))\] <ul> <li><strong>Replay Buffer</strong>: Large enough for stability, impossible to contain everything, trade-off between recent data (overfitting) and old data (slow)</li> <li><strong>Target Networks</strong>: Target is also parameterized by $\phi, \theta \rightarrow$ unstable training, parameterized as $\phi_{targ}, \theta_{targ}$, copy of main network for every fixed number of steps, updated as $\phi_{targ} = \rho\phi_{targ} + (1-\rho)\phi, \theta_{targ} = \rho\theta_{targ} + (1-\rho)\theta$</li> <li><strong>Issues</strong>: Hyperparameter tuning, drastically overestimate Q-value (exploit Q-function errors)</li> <li><strong>Twin Delayed DDPG</strong>: Off-policy, continuous action spaces <ul> <li><strong>Clipped Double-Q Learning</strong>: Learns 2 Q-functions, Uses the smaller ones as the target, reduce further the overestimation</li> <li><strong>Delayed Policy Updates</strong>: Update policy less frequently than Q-function, reduce the volatility when policy update changes the target</li> <li><strong>Target Policy Smoothing</strong>: Added noise to action (form of regularization)</li> </ul> </li> </ul> \[\begin{aligned} Q_{\phi_{targ}}(s',a'(s')) &amp;= Q_{\phi_{targ}}(s', clip(\mu_{\theta_{targ}}(s') + clip(\epsilon,-c,c), a_{Low}, a_{High}))) &amp; \epsilon\sim N(0,\sigma) \end{aligned}\] <ul> <li><strong>SAC</strong>: both continuous and discrete action spaces <ul> <li><strong>Entropy regularized RL</strong>: how random a random variable is, $H(P) = \mathbb E_{x\sim P}[-\log P(x)]$, more reward for high entropy (exploration)</li> <li><strong>Similar to TD3</strong>: Q-functions with MSBE, target Q-networks (polyak averaging), clipped double-Q</li> <li><strong>Different from TD3</strong>: entropy regularization, stochastic policy replaces target policy smoothing, next state-action comes from current policy instead of target policy (replay buffer)</li> <li><strong>Remark</strong>: The objective is no longer the $Q^*$</li> </ul> </li> </ul> \[\begin{aligned} Q^\pi(s,a) &amp;= \mathbb E_{s'\sim P, a'\sim \pi}[R(s,a,s') + \gamma(Q^\pi(s',a') + \alpha H(\pi(.|s')))] &amp; \text{Regularized entropy}\\ &amp;= \mathbb E_{s'\sim P, a'\sim \pi}[R(s,a,s') + \gamma(Q^\pi(s',a') - \alpha \log\pi(a'|s'))] \\ &amp;= r + \gamma (Q^\pi(s',\tilde a') - \alpha \log\pi(\tilde a'|s')) &amp; \text{Current policy, not from replay buffer} \end{aligned}\] <p>\(\begin{aligned} V^\pi(s,a) &amp;= E[Q^\pi(s,a) -\alpha \log\pi(a|s)] \\ &amp;= E_{s\in D,\delta\sim\mathcal N}[Q^\pi(s,\tilde a_\theta(s,\delta)) -\alpha \log\tilde a_\theta(s,\delta)] &amp; \tilde a_\theta(s,\delta) = \tanh(\mu_\theta(s) + \sigma_\theta(s) \odot \delta)\\ \end{aligned}\)</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://spinningup.openai.com/en/latest/user/algorithms.html">https://spinningup.openai.com/en/latest/user/algorithms.html</a></li> <li><a href="https://arxiv.org/pdf/2301.01379.pdf">https://arxiv.org/pdf/2301.01379.pdf</a></li> <li><a href="http://incompleteideas.net/book/RLbook2020.pdf">http://incompleteideas.net/book/RLbook2020.pdf</a></li> </ul>]]></content><author><name>Thang Chu</name></author><category term="RL"/><summary type="html"><![CDATA[A remark of RL from various source (CMPUT 365, paper, blogs, textbook, etc.)]]></summary></entry><entry><title type="html">Statistical learning</title><link href="https://chuducthang77.github.io/blog/2023/stat-441/" rel="alternate" type="text/html" title="Statistical learning"/><published>2023-01-30T00:00:00+00:00</published><updated>2023-01-30T00:00:00+00:00</updated><id>https://chuducthang77.github.io/blog/2023/stat-441</id><content type="html" xml:base="https://chuducthang77.github.io/blog/2023/stat-441/"><![CDATA[<h1 id="statistical-learning">Statistical Learning</h1> <h2 id="terminology">Terminology</h2> <ul> <li><strong>Input</strong>: predictors, independent variables, features, variables</li> <li><strong>Output</strong>: responses, dependent variables</li> </ul> <h2 id="prediction">Prediction</h2> <ul> <li>Predict \(\hat Y = \hat f(X)\)</li> <li>The accuracy of \(\hat Y\) depends on the <em>reducible error</em> and <em>irreducible error</em></li> <li><strong>Reducible error</strong>: The difference between \(\hat f\) and true \(f\)</li> <li><strong>Irreducible error</strong>: \(f\) also depends on \(\epsilon\), unmeasurable, unknown</li> </ul> \[\mathbb E[(Y - \hat Y)^2] = (f(X) - \hat f(X))^2 + \text{Var}[\epsilon]\] <h2 id="inference">Inference</h2> <ul> <li>Which predictors are associated with Y?</li> <li>What is the relationship between Y and each predictor?</li> <li>Is a linear equation adequate to represent the relationship between Y and predictors?</li> </ul> <h2 id="how-to-estimate-f">How to estimate f</h2> <ul> <li><strong>Parametric</strong>: Make <strong>assumption about functional form</strong>, uses training data to fit the model, easier to estimate a set of parameters, \(\hat f\) may be far from \(f\).</li> <li><strong>Non-parametric</strong>: No assumption about functional form, estimate f that close to data points without being too rough or wiggly, requires a very large number of observations.</li> </ul> <h2 id="accuracy-vs-interpretability-vs-flexibility">Accuracy vs Interpretability (vs Flexibility)</h2> <ul> <li>More flexible = (maybe) more accurate = Less interpretability</li> </ul> <h2 id="assess-model-accuracy">Assess model accuracy</h2> <ul> <li><strong>No Free Lunch theorem</strong>: no method dominates all others over all possible data sets.</li> <li><strong>Measuring the Quality of Fit</strong>: MSE for regression, error rate for classification.</li> <li><strong>Training vs Testing error</strong>: Monotonic decreasing and U-shape, training error is always smaller since we directly optimize.</li> <li><strong>Overfitting vs Underfitting</strong>: U-shape testing error, both training and testing error are high.</li> </ul> <h2 id="bias-variance-trade-off">Bias-Variance Trade-off</h2> \[\mathbb E[(y_0 - \hat f(x_0))^2] = \text{Var}[\hat f(x_0)] + \text{Bias}(\hat f(x_0))^2 + \text{Var}[\epsilon]\] <ul> <li><strong>Remark</strong>: Expected MSE cannot lie below \(Var[\epsilon]\)</li> <li><strong>Remark</strong>: We want to achieve <strong>simultaneously</strong> low bias and low variance</li> <li><strong>Variance</strong>: How \(\hat f\) changes if we shift one of the data point</li> <li><strong>Bias</strong>: Error introduced by approximating a real-life problem <ul> <li><strong>Remark</strong>: High flexible = High variance = Less bias</li> <li><strong>Remark</strong>: The rate of changing bias and variance matters</li> </ul> </li> </ul> <h2 id="the-bayes-classifier">The Bayes classifier</h2> <ul> <li>Assigns each observation to most likely class given its predictor values</li> <li><strong>Bayes error rate</strong>: \(1 - \max_j \Pr(Y = j \|X = x_0)\), analogous to irreducible error <ul> <li>Gold standard to compare other methods</li> </ul> </li> <li> <p><strong>K-nearest neighbor</strong>: Unknown \(P(Y = j \| X= x_0)\)</p> \[\max_{j} \Pr(Y = j \| X = x_0 ) = \frac{1}{k}\sum_{i\in N_0} I(y_i = j)\] <ul> <li><strong>Remark</strong>: Close to optimal Bayes classifier</li> <li><strong>Remark</strong>: Selecting K matters (bias-variance trade-off)</li> </ul> </li> </ul> <h1 id="linear-regression">Linear Regression</h1> <h2 id="question">Question</h2> <ul> <li>Is there a relationship between response and predictors?</li> <li>Is the relationship linear?</li> <li>How strong the relationship is?</li> <li>Is there an interaction (synergy) effect among predictors?</li> <li>How accurately our prediction is?</li> </ul> <h2 id="formulation">Formulation</h2> \[Y = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p + \epsilon\] <ul> <li><strong>Assumption</strong>:\(\epsilon\):\(\perp\!\!\!\!\perp X\) , \(\epsilon_i \perp\!\!\!\!\perp \epsilon_j\), catch-all we miss (nonlinear relationship, missing predictors, measurement error)</li> <li><strong>Residual</strong>: \(e_i = y_i - \hat y_i\)</li> <li><strong>Objective</strong>: \(\min RSS = e_1^2 + ... + e_n^2\)</li> <li><strong>Estimating coefficients</strong>: Least squares solution</li> <li><strong>Interpretation</strong>: <ul> <li>\(\hat\beta_0\): Average value of Y</li> <li>\(\hat\beta_j\): Average increase in Y if we increase \(X_j\) by 1</li> </ul> </li> </ul> <h2 id="simple-linear-regression">Simple Linear Regression</h2> <ul> <li> <p>Number of predictor \(p = 1\)</p> </li> <li> <p><strong>Assessing model</strong>:</p> <ul> <li> <p><strong>Residual standard error</strong>: measure lack of fit, an estimate of standard deviation of \(\epsilon\)</p> \[\hat\sigma = RSE = \sqrt{\frac{RSS}{n-p-1}}\] </li> <li> <p><strong>\(R^2\)-statistics</strong>: absolute measure of lack of fit, proportion of variability explained by regression</p> \[0 \le R^2 = 1 - \frac{RSS}{TSS} \le 1\] <ul> <li><strong>RSS</strong>: amount of variability left after performing regression</li> <li><strong>TSS</strong>: total variance in response Y - amount of variability before regressing (only \(\bar y\))</li> <li>Interpretational advantage over RSE, but what is <em>good</em> \(R^2\)?</li> <li><strong>Remark</strong>: \(R^2 = r^2\) (correlation)</li> </ul> </li> <li> <p><strong>Others</strong>: confidence interval, hypothesis testing, p-value</p> </li> </ul> </li> </ul> <h2 id="multiple-linear-regression">Multiple Linear Regression</h2> <ul> <li>Number of predictor \(p &gt; 1\)</li> <li><strong>Is there relationship between response and predictors?</strong>: Hypothesis testing (F-test) <ul> <li>All predictors</li> <li><strong>Linearity assumption satisfies</strong>: \(E[\frac{RSS}{n-p-1}] = \sigma^2\)</li> <li><strong>\(H_0\)</strong>: \(E[\frac{RSS}{n-p=1}]=\sigma^2\)</li> <li><strong>Remark</strong>: Test all together, instead of individual</li> </ul> </li> <li><strong>Which predictors are important?</strong> <ul> <li><strong>Criteria</strong>: Mallow’s \(C_p\), AIC, BIC, Adjusted-\(R^2\)</li> <li><strong>Procedure</strong>: Forward selection, backward selection, or mixed</li> </ul> </li> <li><strong>How good model fits?</strong>: RSE and \(R^2\)-statistics <ul> <li><strong>Remark</strong>: The relative change between p and RSS</li> <li><strong>Remark</strong>: More variables always increase \(R^2\) (overfitting)</li> </ul> </li> <li><strong>How accurate the prediction?</strong> <ul> <li><strong>Reducible error</strong>: Inaccuracy between \(f(X)\) and \(\hat Y\), model bias <ul> <li><strong>Confidence interval</strong>: interval contains the true value \(f(X)\), uncertainty around average Y over large X</li> </ul> </li> <li><strong>Irreducible error</strong>: Inaccuracy between \(Y\) and \(\hat Y\), random error \(\epsilon\) <ul> <li><strong>Prediction interval</strong>: interval contains the true value of Y, uncertainty around Y over a particular X</li> </ul> </li> </ul> </li> </ul> <h2 id="qualitative-response-variable">Qualitative response variable</h2> <ul> <li><strong>Dummy variable</strong>: incorporating qualitative variable into regression analysis</li> <li><strong>Coding scheme 0/1</strong> (One-hot encoding): <ul> <li>\(\beta_0\): Average Y when X = 0</li> <li>\(\beta_1\): The difference in average Y when X = 0 and X = 1</li> </ul> </li> <li><strong>Coding scheme -1/1</strong>: <ul> <li>\(\beta_0\): Average Y (ignore X)</li> <li>\(\beta_1\): Amount each X have Y that above or below average</li> </ul> </li> <li><strong>Remark</strong>: Coding scheme does not affect the fit</li> <li><strong>More than 2 level</strong>: Always one fewer dummy variable than number of levels <ul> <li>No dummy variable = baseline</li> </ul> </li> </ul> <h2 id="extension-of-linear-model">Extension of linear model</h2> <ul> <li><strong>Remove additive assumption</strong>: <ul> <li>Increase in \(X_j\) associated with one-unit increase in \(X_k\)</li> <li><strong>Hierarchical principle</strong>: If we include the <strong>interaction term</strong>, we also should <strong>include the main effect</strong> even if the p-value associated with the coefficients are <strong>not significant</strong>.</li> </ul> </li> <li><strong>Nonlinear relationship</strong>: Polynomial regression</li> </ul> <h2 id="potential-problems">Potential Problems</h2> <ul> <li><strong>Nonlinear between response and predictors</strong> <ul> <li><strong>Residual plot</strong>: Observe any discerning patterns</li> <li><strong>Solution</strong>: transformation on X</li> </ul> </li> <li><strong>Correlation among error terms</strong> <ul> <li>Underestimate the true standard error (Accidentally double the observations)</li> <li><strong>Very</strong> importance to linear regression</li> </ul> </li> <li><strong>Non-constant variance of error term</strong> <ul> <li><strong>Residual plot</strong>: Funnel shape in residual plot</li> <li><strong>Solution</strong>: transformation on Y</li> </ul> </li> <li><strong>Outliers</strong>: Point where \(y_i\) is unusual given \(x_i\) <ul> <li><strong>Remark</strong>: Outliers can have no effect on least square fit</li> <li><strong>Studentized residuals</strong>: Detect outliers</li> </ul> </li> <li><strong>High-leverage points</strong>: Point where \(x_i\) is unusual <ul> <li><strong>Remark</strong>: Removing high leverage points are important</li> <li><strong>Remark</strong>: High-leverage points are outliers, but not vice versa</li> <li><strong>Leverage statistics</strong>: To detect high leverage points!</li> </ul> \[h_i = \frac{1}{n} + \frac{(x_i - \bar x)^2}{\sum_{i'=1}^n (x_{i'} - \bar x)^2}\] <ul> <li>Between 1 and \(\frac{1}{n}\) and the average value <strong>ALWAYS</strong> equals \(\frac{p+1}{n}\)</li> </ul> </li> <li><strong>Collinearity</strong>: Two or more variables are closely related to each other <ul> <li>Difficult to separate out the individual effects</li> <li>Inaccurate \(\hat\beta_j\) and \(SE[\hat\beta_j]\) increases, power of the hypothesis test reduces</li> <li><strong>Correlation matrix</strong>: Good for pairs of variables</li> <li><strong>Multicollinearity</strong>: Variance inflation factor</li> <li><strong>Solution</strong>: Drop or combine collinear variables together into a single predictor/</li> </ul> </li> </ul> <h2 id="comparsion-with-knn">Comparsion with KNN</h2> <ul> <li>Parametric vs non-parametric</li> <li><strong>Linearity assumption</strong>: LR</li> <li><strong>Non-linearity in low dimension</strong>: LR</li> <li><strong>Non-linearity in high dimension</strong>: KNN</li> <li><strong>Small observations</strong>: LR (Curse of dimensionality)</li> </ul> <h1 id="logistic-regression">Logistic Regression</h1> <h2 id="why-not-linear-regression">Why not Linear Regression</h2> <ul> <li><strong>Coding scheme</strong>: implies outcomes ordering.</li> <li><strong>Linear Regression</strong>: Hard to interpret as probability, and impossible for more than 2 classes</li> </ul> <h2 id="logistic-model">Logistic Model</h2> <ul> <li><strong>Logistic function</strong>: \(p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\)</li> <li><strong>Odds</strong>: \(\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X}\)</li> <li><strong>Logit odds</strong>: \(\log \frac{p(X)}{1-p(X)} = \beta_0 + \beta_1 X\) <ul> <li>\(\beta_1\): Increases X by 1 changes the <strong>logit odds</strong> by \(\beta_1\).</li> </ul> </li> </ul> <h2 id="estimating-coefficients">Estimating coefficients</h2> <ul> <li><strong>Maximum likelihood function</strong>:</li> </ul> \[l(\beta_0, \beta_1) = \prod_{i:y_i = 1}p(x_i) \prod_{i':y_{i'}=0} (1 - p(x_{i'}))\] <ul> <li><strong>Predictions</strong>: Possible to have qualitative predictors (using dummy variables)</li> </ul> <h3 id="multiple-logistic-regression">Multiple Logistic Regression</h3> \[p(Y=1\|X) = p(X) = \frac{e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}\] <h3 id="multinomial-logistic-regression">Multinomial Logistic Regression</h3> <ul> <li><strong>Baseline encoding</strong>:</li> </ul> \[P(Y = k \| X = x) = \frac{e^{\beta_{k0} + \beta_{k1} X_1 + ... + \beta_{kp} X_p}}{1 + \sum_{l=1}^{K-1}e^{\beta_{l0} + \beta_{l1} X_1 + ... + \beta_{lp} X_p}}, \quad \forall k = 1,...,K-1\] \[P(Y = k \| X = x) = \frac{1}{1 + \sum_{l=1}^{K-1}e^{\beta_{l0} + \beta_{l1} X_1 + ... + \beta_{lp} X_p}}, \quad k = K\] <ul> <li> <p><strong>Remark</strong>: The choice of baseline will <strong>change the coefficients</strong>, but the fitted value will be the same.</p> </li> <li> <p><strong>Softmax encoding</strong>: No baseline</p> </li> </ul> \[P(Y = k \| X = x) = \frac{e^{\beta_{k0} + \beta_{k1} X_1 + ... + \beta_{kp} X_p}}{\sum_{l=1}^{K}e^{\beta_{l0} + \beta_{l1} X_1 + ... + \beta_{lp} X_p}}\] <h2 id="generative-models-for-classification">Generative Models for classification</h2> <h3 id="motivation">Motivation</h3> <ul> <li><strong>Condition</strong>: Substantial separation between 2 classes, small sample size, predictors are approximately normal.</li> </ul> \[p_k(x) = P(Y = k \| X= x) = \frac{P(Y = k)P(X = x\|Y = k)}{\sum_{l=1}^K P(Y = l) P(X =x \| Y = l)} = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}\] <ul> <li><strong>Challenge</strong>: estimating \(f_k(x) \rightarrow\) simplifying assumption</li> </ul> <h3 id="linear-discriminant-analysis-p--1">Linear Discriminant Analysis p = 1</h3> <ul> <li><strong>Assumption</strong>: \(f_k(x) \sim N(\mu_k, \sigma_k^2)\) and \(\sigma_1^2 = ... = \sigma_K^2 = \sigma^2\)</li> </ul> \[\max_k p_k(x) =\delta_k(x) = x \times\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)\] <ul> <li><strong>Remark</strong>: Linear in terms of x</li> <li><strong>Challenge</strong>: \(\mu_k\) and \(\sigma^2\) are unknown. Then, estimate</li> </ul> \[\hat\mu_k = \frac{1}{n_k}\sum_{i:y_i = k}x_i\] \[\hat\sigma^2 = \frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i = k}(x_i - \hat\mu_k)^2\] \[\hat\pi_k = \frac{n_k}{n}\] <h3 id="linear-discriminant-analysis-p--1-1">Linear Discriminant Analysis p &gt; 1</h3> <ul> <li><strong>Assumption</strong>: \(f_k(x)\sim N(\mu_k, \Sigma)\)</li> </ul> \[\max_k p_k(x) = \delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log\pi_k\] <ul> <li><strong>Decision boundary</strong>: \(\delta_k(x) = \delta_l(x)\)</li> <li><strong>Remark</strong>: Prefer when more than 2 classes (view data in low dimension)</li> </ul> <h3 id="metric">Metric</h3> <ul> <li><strong>Confusion matrix</strong></li> <li><strong>Sensitivity</strong>: Percent of TP</li> <li><strong>Specificity</strong>: Percent of TN <ul> <li>Bayes classifier does poorly in Specificity since minimizing <em>total</em> error rate</li> <li><strong>Threshold</strong> (in K = 2) affects specificity and sensitivity \(\rightarrow\) domain knowledge</li> </ul> </li> <li><strong>ROC curve</strong>: Overall performance of the classifier, all possible threshold, given by area under curve (AUC) <ul> <li><strong>False positive rate</strong> = 1 - Specificity = Type I error</li> <li><strong>True positive rate</strong> = Sensitivity = Recall = 1 - Type II error = Power</li> <li><strong>Precision</strong> = Among positive prediction, how many actually TP</li> </ul> </li> </ul> <h3 id="quadratic-discriminant-analysis">Quadratic Discriminant Analysis</h3> <ul> <li><strong>Assumption</strong>: \(f_k(x)\sim N(\mu_k, \Sigma_k)\)</li> </ul> \[\max_k p_k(x) = \delta_k(x) = -\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k - \frac{1}{2}\log\|\Sigma_k\| + \log\pi_k\] <ul> <li><strong>Challenge</strong>: High variance \(\rightarrow\) work better with more observations (bias-variance trade-off)</li> </ul> <h3 id="naive-bayes">Naive Bayes</h3> <ul> <li><strong>Assumption</strong>: Among kth class, p predictors are independent, \(f_k(x) = f_{k1}(x_1) \times... \times f_{kp}(x_p)\)</li> <li><strong>Remark</strong>: Works well when n &lt; p (reduces variance)</li> </ul> \[P(Y = k \| X = x) = \frac{\pi_k \times f_{k1}(x_1) \times ...\times f_{kp}(x_p)}{\sum_{l=1}^K \pi_l \times f_{l1}(x_1) \times ... \times f_{lp}(x_p)}\] <ul> <li>If \(X_j\) is quantitative, \(f_{kj}(x_j) \sim N(\mu_{kj}, \sigma^2_{kj})\) (Similar to QDA with diagonal covariance matrix) or nonparametric (kernel density estimator)</li> <li>If \(X_j\) is qualitative, count the proportion of training obs for jth predictor corresponding to each class</li> </ul> <h2 id="comparison-of-all-methods">Comparison of all methods</h2> <ul> <li>NB takes a form generalized additive model</li> <li>LDA is a special case of QDA</li> <li>Any linear boundary classifiers are special cases of NB with \(g_{kj} = b_{kj}x_j\)</li> <li>NB when \(X_j\) is quantitative and \(f_{kj}(x_j) \sim N(\mu_{kj}, \sigma^2_{kj})\) are special case of QDA (\(\Sigma\) is a diagonal matrix)</li> <li>LDA will be better than Logistic Regression when Normal assumption does not hold.</li> <li>KNN is better when decision boundary is highly non-linear and n » p. If the relationship is non-linear and n \(\approx\) p, QDA is prefer.</li> </ul> <h2 id="generalized-linear-model">Generalized Linear Model</h2> <ul> <li>Y belongs to <strong>Exponential Family Distribution</strong></li> <li><strong>Poisson Regression</strong>: \(\lambda(X_1,...,X_p) = e^{\beta_0 + \beta_1X_1 + ... + \beta_pX_p}\) and use MLE to find \(\hat\beta\) <ul> <li><strong>Interpretation</strong>: Increases \(X_j\) by one unit with a change in \(\mathbb E[Y]\) by \(\exp(\beta_j)\)</li> <li><strong>Mean variance relationship</strong>: \(Var[Y] = \mathbb E[Y] = \lambda\)</li> <li><strong>Nonnegative</strong> fitted values</li> </ul> </li> <li><strong>Link function</strong>: Transform mean of the response so that the transformed mean is a linear function of predictors</li> </ul> <h1 id="sampling-method">Sampling Method</h1> <ul> <li><strong>Model selection</strong>: Select proper level of flexibility</li> <li><strong>Model assessment</strong>: Estimate the test error</li> </ul> <h2 id="cross-validation">Cross-validation</h2> <h3 id="validation-set-approach">Validation Set approach</h3> <ul> <li>Randomly divided train and valdiation set</li> <li><strong>Challenge</strong>: Overestimate and highly variable test error rate</li> <li>High bias (overestimation), High variance</li> </ul> <h3 id="loocv">LOOCV</h3> <ul> <li> <p>Leave one as validation and the rest as training set. Repeat n times</p> \[CV_{(n)} = \frac{1}{n}\sum_{i=1}^n MSE_{i}\] </li> <li><strong>Benefits</strong>: Unbiased estimate of test error rate, less variance and bias in train/valid split.</li> <li><strong>Challenge</strong>: expensive</li> <li><strong>Remark</strong>: Less expensive for LSS or polynomial regression (exact solution)</li> </ul> <h3 id="k-fold-cv">k-fold CV</h3> <ul> <li> <p>Randomly divide the dataset into k groups</p> \[CV_{(k)} = \frac{1}{k}\sum_{i=1}^k MSE_i\] </li> <li>LOOCV is a <strong>special</strong> case</li> <li><strong>Benefits</strong>: Less expensive, more accurate estimation of test error rate</li> <li><strong>Challenge</strong>: More variance (more random in split), more bias (less in training)</li> <li><strong>Classification</strong>: Similar idea, but can sometimes underestimate the test error rate!</li> </ul> <h2 id="bootstrap">Bootstrap</h2> <ul> <li>Apply when difficult to obtain a measure of variability.</li> <li><strong>Problem</strong>: Data cannot be generated from original population.</li> <li><strong>Solution</strong>: <strong>Repeated sampling (with replacement)</strong> observations from the <em>original</em> dataset \(\rightarrow\) <strong>obtain the estimation</strong> and <strong>use the formula</strong> to obtain the standard error of the estimation.</li> </ul> <h1 id="regularization">Regularization</h1> <h2 id="motivation-1">Motivation</h2> <ul> <li>Instead of using LS model</li> <li><strong>Model interpretability</strong>: Irrelevant variables leads to unnecessary complexity, which makes harder to interpret.</li> </ul> <h2 id="subset-selection">Subset selection</h2> <ul> <li><strong>Best Subset selection</strong>: \(2^p - 1\) models <ul> <li>For each k (0,…, p-1) predictors \(\rightarrow\) Fit \(p - k\) models with one more predictors \(\rightarrow\) Choose best model (smallest RSS or highest \(R^2\), or deviance), called \(M_k\) \(\rightarrow\) Choose the best model among \(M_k\) (k=1,…,p) (cross-validation error, \(C_p\), BIC, or adjusted-\(R^2\))</li> <li><strong>Challenge</strong>: expensive</li> </ul> </li> </ul> <h2 id="stepwise-selection">Stepwise selection</h2> <ul> <li><strong>Motivation</strong>: Large search space \(\rightarrow\) overfitting and high variance in the estimate</li> <li><strong>Forward</strong>: <ul> <li>For each k (1,…, p) predictors \(\rightarrow\) Fit \(p - k\) models with one additional models \(\rightarrow\) Choose best model (smallest RSS or highest \(R^2\), or deviance), called \(M_k\) \(\rightarrow\) Choose the best model among \(M_k\) (k=1,…,p) (cross-validation error, \(C_p\), BIC, or adjusted-\(R^2\))</li> <li>Do well in <strong>practice</strong></li> <li><strong>Not guarantee</strong> to find best model</li> <li>Possible to apply when n &lt; p</li> </ul> </li> <li><strong>Backward</strong>: <ul> <li>For each k (p, p-1, …, 1) predictors \(\rightarrow\) Fit \(k\) models with one less predictor than previously \(\rightarrow\) Choose best model (smallest RSS or highest \(R^2\), or deviance), called \(M_k\) \(\rightarrow\) Choose the best model among \(M_k\) (k=1,…,p) (cross-validation error, \(C_p\), BIC, or adjusted-\(R^2\))</li> <li>Requires n &gt; p</li> </ul> </li> <li><strong>Hybrid</strong>: Combination of two previous ones</li> </ul> <h2 id="choosing-the-optimal-model"><strong>Choosing the optimal model</strong></h2> <ul> <li><strong>Indirectly estimate testing error</strong>: adjust training error <ul> <li>Assumption about true underlying model</li> <li>\(C_p\): \(\frac{1}{n}(RSS + 2d\hat\sigma^2)\) <ul> <li>\(\hat\sigma^2\): estimate of \(\epsilon\) (full-model)</li> <li>More features (d) will increase \(C_p\)</li> <li>\(E[C_p]\) = Test MSE (unbiased)</li> </ul> </li> <li>\(AIC\): \(\frac{1}{n}(RSS + 2d\hat\sigma^2)\) <ul> <li>Class of models fit by MLE</li> <li>Proportional to \(C_p\)</li> </ul> </li> <li>\(BIC\): \(\frac{1}{n}(RSS + \log(n)d\hat\sigma^2)\) <ul> <li>Bayesian POV</li> <li>Heavier penalty for large model</li> </ul> </li> <li>Adjusted-\(R^2\): \(1\frac{RSS/(n-d-1)}{TSS/(n-1)}\) <ul> <li>Adding more features reduces RSS, and reduces (n-d-1). Therefore, the relative change matters.</li> <li>Not well motivated in statistical theory</li> <li>Large value is better (Opposite with above three criterion)</li> </ul> </li> </ul> </li> <li><strong>Directly estimate testing error</strong>: validation set approach or cross-validation <ul> <li>Fewer assumption about true underlying model</li> <li>Prefer when it is hard to pinpoint df and \(\sigma^2\)</li> </ul> </li> </ul> <h2 id="shrinkage">Shrinkage</h2> <ul> <li><strong>Ridge regression</strong>:</li> </ul> \[RSS + \lambda\sum_{j=1}^p \beta_j^2\] <ul> <li>\(\lambda\): shrinkage penalty, shrink \(\beta_j\) toward zero</li> <li><strong>Remark</strong>: shrinkage penalty is not applied to \(\beta_0\) (measure of the mean value of the response)</li> <li><strong>Remark</strong>: \(\lambda\) increases may increase estimated coefficients</li> <li><strong>Remark</strong>: LS solution is scale equivariant, while RR coeff is not.</li> <li><strong>Remark</strong>: Standardizing features before applying ridge regression</li> <li><strong>Bias-variance trade-off</strong>: Increase bias, reduce variance (flexibility)</li> <li><strong>Remark</strong>: Shrink coefficients towards, but not exactly, zero</li> <li>\(\min_\beta \|\|Y - X\beta\|\|^2\) subject to \(\sum_{j=1}^p \beta_j^2 \le s\)</li> <li> <p><strong>Remark</strong>: Normal distribution prior and follows by posterior mode/mean for \(\beta\)</p> </li> <li><strong>LASSO</strong>:</li> </ul> \[RSS + \lambda\sum_{j=1}^p \|\beta_j\|\] <ul> <li><strong>Model interpretation/Variable selection</strong>: Forcing some coefficients to be exactly zero</li> <li><strong>Sparse model</strong>: \(\lambda\) is sufficiently large</li> <li>\(\min_\beta \|Y - X\beta\|^2\) subject to \(\sum_{j=1}^p \|\beta_j\| \le s\)</li> <li><strong>Closely related to Best Subset selection</strong>: \(\min_\beta \|Y - X\beta\|^2\) subject to \(\sum_{j=1}^p I(\beta_j\not = 0) \le s\)</li> <li><strong>Remark</strong>: Corner solution</li> <li><strong>Remark</strong>: Neither LASSO nor Ridge regression universally dominates the other</li> <li><strong>Remark</strong>: Since the derivative of absolute function does not exist at 0, we need <strong>soft thresholding</strong> (explicitly set coefficients to be 0)</li> <li><strong>Remark</strong>: Laplace distribution prior and follows posterior mode (not mean)</li> </ul> <h2 id="dimension-reduction-to-be-written-in-the-near-future">Dimension reduction: To be written in the near future</h2> <h2 id="consideration-in-high-dimension-to-be-written-in-the-near-future">Consideration in high dimension: To be written in the near future</h2> <h1 id="beyond-linearity">Beyond Linearity</h1> <h2 id="polynomial-regression"><strong>Polynomial regression</strong></h2> \[y_i = \beta_0 + \beta_1 x_i + ... +\beta_d x_i^d +\epsilon\] <ul> <li>Extremely non-linear curve</li> <li><strong>Remark</strong>: d &lt; 5, otherwise overly flexible and strange shapes</li> <li><strong>Variance of the fit</strong>: point-wise square-root of the variance estimate for each coefficients</li> <li>Applicable for linear and logistic regression</li> </ul> <h2 id="step-functions"><strong>Step functions</strong>:</h2> \[y_i = \beta_0 + \beta_1 C_1(x_i) + ... + \beta_K C_K(x_i) + \epsilon\] <ul> <li>Global structure on the non-linear function</li> <li>Continuous variable \(\rightarrow\) ordered categorical variable</li> <li>\(C_K(x) = I(c_K \le x \le c_{K+1})\): indicator function, dummy variable</li> <li>\(\beta_0\): Mean value of Y for \(X &lt; c_1\)</li> <li>\(\beta_j\): Average increase in Y for X in \(c_j \le X &lt; c_{j=1}\) relative to \(X &lt; c_1\)</li> <li><strong>Remark</strong>: Unless natural breakpoints, miss the action</li> </ul> <h2 id="basis-functions"><strong>Basis functions</strong>:</h2> \[y_i = \beta_0 + \beta_1 b_1(x_i) + ... + \beta_K b_K(x_i) + \epsilon\] <ul> <li>\(b_k\): fixed and known function (Step and polynomial regression are special case of this)</li> <li>Applicable for OLS</li> </ul> <h2 id="regression-splines"><strong>Regression splines</strong>:</h2> <ul> <li><strong>Piecewise Polynomials</strong>: \(y_i = \beta_{01} + \beta_{11}x_i + \beta_{21}x_i^2 + \epsilon\) if \(x_i &lt; c\) otherwise \(\beta_{02} + \beta_{12} x_i + \beta_{22}x_i + \epsilon\) <ul> <li><strong>Knots</strong>: c, more knots = more flexible</li> <li><strong>Remark</strong>: Discontinuous (too flexible)</li> <li><strong>Remark</strong>: 1 knot and 4 parameters \(\rightarrow\) 8 parameters in total</li> </ul> </li> <li><strong>Constraints</strong>: first up to K-1 order derivative must be continuous <ul> <li><strong>Remark</strong>: Every constraint frees one degree of freedom</li> <li><strong>Remark</strong>: Cubic splines has K (knots) + 4 (\(\beta_0, \beta_1, \beta_2, \beta_3\)) degrees of freedom</li> </ul> </li> <li><strong>Spline Basis Representation</strong>: <ul> <li><strong>Truncated Power basis</strong> (cubic spline): \(h(x, \xi) = (x - \xi)_+^3\) with \(\xi\) is the location of the knot</li> <li><strong>Remark</strong>: Discontinuity in third derivative</li> <li><strong>Remark</strong>: Higher variance at outer range</li> </ul> </li> <li><strong>Natural spline</strong>: Additional linearity boundary constraints</li> <li><strong>Choosing the number and locations of the knots</strong> <ul> <li><strong>Remark</strong>: More knots = more flexible = coefficients change rapidly</li> <li><strong>Uniform fashion</strong></li> </ul> </li> <li><strong>Compare with Polynomial Regression</strong> <ul> <li>Different ways to introduce flexibility \(\rightarrow\) more stable</li> </ul> </li> </ul> <h2 id="smoothing-splines"><strong>Smoothing splines</strong>:</h2> <p>g = \(\arg\min_g \sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2dt\)</p> <ul> <li><strong>Remark</strong>: g is <strong>VERY</strong> flexible (interpolates all \(y_i\) makes RSS = 0) - <strong>Roughness</strong>: second derivative = how fast first derivative change - <strong>Remark</strong>: Integration = total change in the first derivative - \(\lambda\): \(\lambda \rightarrow \infty\), g becomes very smooth - <strong>Remark</strong>: shrunken version of natural cubic spline - <strong>Remark</strong>: As \(\lambda\) increases \(0 \rightarrow \infty\), \(df_{\lambda}\) decreases \(n\rightarrow 2\) - <strong>Effective degrees of freedom</strong>: measure the flexibility of the spline</li> </ul> <h2 id="local-regression"><strong>Local regression</strong>:</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Compute the fit at target point using nearby training observations
- $$K_{i0}$$ different for each value of $$x_0
- **Memory-based**: Similar to KNN (need neighbor to compute)
- **s**: span, proportion of points to compute local regression, similar to $$\lambda
  - **Remark**: smaller = more wiggly
- Poor performance in high dimension (Curse of dimensionality)
</code></pre></div></div> <h2 id="generalized-additive-models-to-be-written-in-the-future"><strong>Generalized additive models</strong>: To be written in the future</h2> <h1 id="tree-based">Tree-based</h1> <ul> <li><strong>Basics of Decision Trees</strong></li> <li><strong>Regression Trees</strong> <ul> <li><strong>Stratification of Feature Space</strong></li> <li><strong>Tree Pruning</strong></li> </ul> </li> <li><strong>Classification Trees</strong></li> <li><strong>Trees vs Linear models</strong></li> <li><strong>Advantages and Disadvantages</strong></li> <li><strong>Bagging, RF, Boosting, Bayesian Additive Regression Trees</strong> <ul> <li><strong>Bagging</strong> <ul> <li><strong>Out-of-Bag error estimation</strong></li> <li><strong>Variable Important measures</strong></li> </ul> </li> <li><strong>RF</strong></li> <li><strong>Boosting</strong></li> <li><strong>Bayesian Additive Regression Trees</strong>: To be added in the future</li> </ul> </li> </ul> <h1 id="svm">SVM</h1> <ul> <li><strong>Maximal Margin Classifier</strong> <ul> <li><strong>Hyperplane</strong></li> <li><strong>Classification using separating hyperplane</strong></li> <li><strong>The classifier</strong></li> <li><strong>Construction of the classifier</strong></li> <li><strong>Non-separable cases</strong></li> </ul> </li> <li><strong>Support Vector Classifier</strong> <ul> <li><strong>Overview</strong></li> <li><strong>Detail</strong></li> </ul> </li> <li><strong>Support Vector Machines</strong> <ul> <li><strong>Non-linear Decision Boundaries</strong></li> <li><strong>More than 2 cases</strong> <ul> <li><strong>One vs one</strong></li> <li><strong>One vs all</strong></li> </ul> </li> <li><strong>Relationship to logistic regression</strong></li> </ul> </li> </ul> <h1 id="neural-network">Neural Network</h1> <h1 id="unsupervised-learning">Unsupervised Learning</h1>]]></content><author><name></name></author><category term="STAT-441"/><category term="ml"/><category term="stat"/><category term="ml"/><category term="stat"/><summary type="html"><![CDATA[A note of my class STAT 441 - Statistical methods for learning and data mining]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://chuducthang77.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://chuducthang77.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://chuducthang77.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>