<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Reinforcement Learning | Thang Duc Chu</title> <meta name="author" content="Thang Duc Chu"> <meta name="description" content="A remark of RL from various source (CMPUT 365, paper, blogs, textbook, etc.)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://chuducthang77.github.io/blog/2023/rl/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Reinforcement Learning",
      "description": "A remark of RL from various source (CMPUT 365, paper, blogs, textbook, etc.)",
      "published": "February 3, 2023",
      "authors": [
        {
          "author": "Thang Chu",
          "authorURL": "",
          "affiliations": [
            {
              "name": "UofA",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Thang </span>Duc Chu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Reinforcement Learning</h1> <p>A remark of RL from various source (CMPUT 365, paper, blogs, textbook, etc.)</p> </d-title><d-byline></d-byline><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#fundamental-concept-of-rl">Fundamental concept of RL</a></div> <div><a href="#bandit">Bandit</a></div> <div><a href="#dynamic-programming">Dynamic Programming</a></div> <div><a href="#monte-carlo">Monte Carlo</a></div> <div><a href="#temporal-difference">Temporal Difference</a></div> <div><a href="#learning-planning-dyna-q">Learning/Planning Dyna-Q</a></div> <div><a href="#function-approximation">Function approximation</a></div> <div><a href="#policy-optimization">Policy optimization</a></div> <div><a href="#deep-rl">Deep RL</a></div> </nav> </d-contents> <p>This page is in the process of editing. Some details and references are still missing. Please forgive me!!</p> <h2 id="fundamental-concept-of-rl">Fundamental concept of RL</h2> <p>This section includes notations (different versions) as well as important formulas. Derivations and specific details will be discussed in later sections.</p> <ul> <li> <strong>Remark</strong>: Please note that any characters with subscript t or capital letter will be random variables</li> <li> <strong>Reinforcement Learning vs Optimal Control vs Operational Research vs Economics</strong>: <ul> <li> <strong>Optimal Control</strong>: Control theory</li> <li> <strong>Operational Research</strong>: Stochastic shortest path</li> <li> <strong>Economics</strong>: Sequential decision making under uncertainty</li> </ul> </li> <li> <strong>Agent</strong>: receives $o_t$ and $r_t$, emit $a_t$</li> <li> <strong>Environment</strong>: Agent lives and interact with, receives $a_t$, emits $o_t$ and $r_{t+1}$</li> <li> <strong>Markov property</strong>: $P(s_{t+1} | s_1, a_1, …, s_t, a_t) = P(s_{t+1} |s_t, a_t)$</li> <li> <strong>Reward</strong>: $r_t=R(s_t, a_t, s_{t+1})$ or $R(s_t,a_t)$ or $R(s_t)$ <ul> <li> <strong>Definition</strong>: Scalar signal received from the environment, estimation of the current state</li> <li> <strong>Matrix format</strong>: $\textbf r\in\mathbb R^{|\mathcal S||\mathcal A| \times 1}$</li> <li> <strong>Deterministic</strong> vs <strong>stochastic</strong> (a random variable with zero mean and non-zero variance)</li> <li> <strong>Bounded reward</strong>: $[0, R_{max}]$</li> <li> <strong>Remark</strong>: agent does not control the reward it receives or randomness of the env</li> <li> <strong>Reward hypothesis</strong>: All goals can be described by the maximization of expected cumulative reward</li> <li> <strong>Reward function</strong>: $R(s_t = s, a_t = a) = \mathbb E[R_t | s_t= s, a_t = a]= \sum_{r\in \mathbb R}\sum_{s'\in S} r P(s',r | s,a)$ <ul> <li>$R(s_t = s, a_t = a, s_{t+1} = s') = \mathbb E[R_t | s_t= s, a_t = a, s_{t+1} = s'] = \sum_{t=0} \sum_{r\in\mathbb R} \gamma^t r \frac{P(s', r|s, a)}{P(s'|s, a)}$</li> </ul> </li> </ul> </li> <li> <strong>Return</strong>: $G_t$ or $R(\tau)$ <ul> <li> <strong>Definition</strong>: cumulative reward, goal of the agent (maximize)</li> <li> <strong>Finite-horizon undiscounted return</strong>: $R(\tau) = \sum_{t=0}^T r_t = \sum_{t=0}^T R(s_t, a_t) $</li> <li> <strong>Infinite-horizon discounted return</strong>: $R(\tau) = \sum_{t=0}^{\infty} \gamma^tr_t = \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)$ (mathematically convenient, but not implement in practice)</li> <li> <strong>Expected Return</strong>: <ul> <li> <strong>Continuous state action space</strong>:$J(\pi) = \mathbb E_{\tau\sim\pi,P,\rho_0}[R(\tau)] = \int_{\tau} P(\tau|\pi)R(\tau) = \int_{\tau} \rho_0(s_0)\prod_{t=0}^{T-1}P(s_{t+1}|s_t,a_t)\pi(a_t|s_t)R(\tau)$</li> <li> <strong>Discrete state action space</strong>: $J(\pi) = \mathbb E_{\tau\sim\pi,P,\rho_0}[R(\tau)] = \sum_{s} \sum_{a} P(s_t = s|\rho_0, \pi, P)\pi(a_t = a|s_t= s) r(s_t = s,a_t = a)$</li> </ul> </li> </ul> </li> <li> <strong>State</strong>: $s_t$ <ul> <li> <strong>Definition</strong>: Complete description of the state, no hidden information from the world, Markov property</li> <li> <strong>State transition</strong>: deterministic ($f(s_t,a_t)$) vs stochastic ($s_{t+1} \sim P(.|s_t,a_t)$)</li> </ul> </li> <li> <strong>History</strong>: $\mathcal H_t = a_1, o_1, r_1,…, a_t, o_t, r_t$, $f(\mathcal H_t) = s_t^\alpha \approx s_t$</li> <li> <strong>Observation</strong>: $o_t$, partial description of a state, generally not Markov property</li> <li> <strong>Observability</strong>: $s_t \equiv o_t$</li> <li> <strong>Action</strong>: $a_t$, discrete (left/right) vs continuous (speeding)</li> <li> <strong>Trajectories</strong>: $\tau = (s_0, a_0, s_1, a_1, …)$, episodes or rollouts</li> <li> <strong>Task</strong>: Continuing (infinite T) and discrete (finite T)</li> <li> <strong>Markov processes</strong>: $\langle \mathcal S, P \rangle$ <ul> <li>$P: \mathcal S \rightarrow \Delta(\mathcal S)$</li> </ul> </li> <li> <strong>Markov reward processes</strong>: $\langle \mathcal S, P, R, \gamma\rangle$ <ul> <li>$R: \mathcal S \rightarrow \mathbb{R}$</li> <li>$P: \mathcal S \rightarrow \Delta(\mathcal S)$</li> </ul> </li> <li> <strong>Markov decision processes</strong>: $\langle \mathcal S, \mathcal A, R, P,\gamma, \rho_0\rangle$ <ul> <li>$R: \mathcal S\times \mathcal A\times \mathcal S \rightarrow \mathbb{R}$</li> <li>$P:\mathcal S\times\mathcal A\rightarrow \Delta(\mathcal S)$</li> </ul> </li> <li> <strong>Partially Observable Markov Decision Process</strong>: $\langle \mathcal S, \mathcal A, \mathcal O, P, R, \mathcal Z, \gamma, \rho_0\rangle$ <ul> <li>$\mathcal O$: Finite set of observations</li> <li>$\mathcal Z$: Observation function</li> <li>Belief state $b(h) = (P(s_t = s_1 | H_t = h), … , P(s_t = s_{|\mathcal S |}) | H_t = h)$</li> </ul> </li> <li> <strong>Probability transition</strong>: <ul> <li> <strong>Four-argument dynamics</strong>: $P(s', r | s, a) = \Pr(s_{t+1} = s', R_t = r | s_t = s, a_t =a )$</li> <li> <strong>Three-argument dynamics</strong>: $P(s' | s, a) = \sum_{r} \Pr(s_{t+1} = s', R_t = r | s_t = s, a_t =a )$</li> <li> <strong>Matrix format</strong>: $\textbf P \in\mathbb [0,1]^{|\mathcal S||\mathcal A|\times | \mathcal S|}$</li> </ul> </li> <li> <strong>Policy</strong>: $\pi$ <ul> <li> <strong>Definition</strong>: Rules used by agents</li> <li> <strong>Matrix format</strong>: $\Pi \in [0,1]^{| \mathcal S |\times | \mathcal S| | \mathcal A |}$ = $diag(\pi(a|s_1) \pi(a|s_2) … \pi(a|s_{| \mathcal S|}))$</li> <li> <strong>Non-stationary</strong> :$\mathcal H \rightarrow \mathcal A$ (Dependent on time t)</li> <li> <strong>Stationary</strong>: $\mathcal S \rightarrow \Delta(\mathcal A)$ or $\mathcal S \rightarrow \mathcal A$ (Non-dependent on time t) <ul> <li> <strong>Deterministic</strong>: $a=\mu(s)$ or $a=\mu_\theta(s)$, MLP</li> <li> <strong>Stochastic</strong>: $a\sim\pi(.|s)$ or $a\sim\pi_\theta(.|s)$, sample from $\pi(.|s)$ and compute $\log\pi(a|s)$ <ul> <li> <strong>Categorical</strong>: discrete, MLP + softmax, argmax to choose the function</li> <li> <strong>Diagonal Gaussian</strong>: continuous, reparameterization trick, $a = \mu_\theta(s) + \sigma_\theta(s) \odot z$ with $z\sim \mathbb{N}(0,I)$</li> </ul> </li> </ul> </li> <li> <strong>Optimal policy</strong>: $\pi^* = \arg\max_\pi J(\pi)$, multiple optimal policies, but same value function</li> <li> <strong>Better policy</strong>: $\pi' \ge \pi$ iff $V^\pi(s) \ge V^{\pi'}(s)\quad\forall s\in \mathcal S$</li> <li> <strong>Greedy policy</strong>: $\pi_Q(s) = \arg\max_a Q(s,a)$</li> <li>$\epsilon$<strong>-optimal policy</strong>: $V^\pi(s) \ge V^*(s) - \epsilon\textbf 1$</li> </ul> </li> <li> <strong>Values</strong> <ul> <li> <strong>(On-policy) Value function</strong>: $V^{\pi}(s) = \mathbb E_{\tau \sim \rho_0, P, \pi}[R(\tau)|s_0=s] $, starts at s, then follow $\pi$ <ul> <li> <strong>Definition</strong>: An estimation of how good it is for an agent to be in a state</li> <li> <strong>Matrix format</strong>: $\textbf V^\pi \in \mathbb R^{|\mathcal S|\times 1}$</li> </ul> </li> <li> <strong>(On-policy) Action-value function</strong>: $Q^{\pi}(s,a) = \mathbb E_{\tau\sim\rho_0, P, \pi}[R(\tau)|s_0=s, a_0=a]$, starts at s, random action a (not follow $\pi$), follow $\pi$ <ul> <li> <strong>Definition</strong>: An estimation of how good it is for an agent to be in a state and taking an action</li> <li> <strong>Matrix format</strong>: $\textbf Q^\pi \in \mathbb R^{|\mathcal S||\mathcal A|\times 1}$</li> </ul> </li> <li> <strong>Optimal value function</strong>: $V^* (s) = \max_\pi V^{\pi}(s) = \max_a Q^*(s,a)$, starts at s, then follow $\pi^{*}$</li> <li> <strong>Optimal action-value function</strong>: $Q^*(s,a) = \max_\pi Q^{\pi}(s,a) = \mathbb E_{s' \sim P(.|s,a)} [ r(s,a,s') + \gamma V^{*}(s') ]$, starts at s, random action a (not follow $\pi^{*}$), then follow $\pi^{*}$</li> <li> <strong>Optimal action</strong>: $a^* = \pi^{*}(a| s) = \arg\max_a Q^* (s,a)$, maybe multiple optimal actions</li> </ul> </li> <li> <strong>Bellman Equations</strong>: <ul> <li> <strong>(On-policy) Value function</strong>: $V^{\pi}(s) = \mathbb E_{a\sim \pi(.|s),s'\sim P(.|s, a)} [r(s,a,s') + \gamma V^{\pi}(s')] = Q(s, \pi(s)) =\mathbb E_{a\sim \pi(.|s)} [ Q^\pi(s,a) ] = \sum_{a}Q^\pi(s,a)\pi(a|s)$ <ul> <li> <strong>Matrix format</strong>: $\textbf V^\pi = \Pi\textbf r + \gamma\Pi\textbf P\textbf V^\pi $</li> </ul> </li> <li> <strong>(On-policy) Action-value function</strong>: $Q^\pi(s,a) = \mathbb E_{s'\sim P(.|s, a)}[r(s,a,s') + \gamma \mathbb E_{a'\sim \pi(.|s)}[Q^\pi(s',a')]] = \mathbb E_{s'\sim P(.|s, a)}[r(s,a,s') + \gamma V^\pi(s')]]$ <ul> <li> <strong>Matrix format</strong>: $\textbf Q^\pi = \textbf r + \gamma \textbf P \Pi \textbf Q^\pi = \textbf r + \gamma \textbf P \textbf V^\pi $ <ul> <li> <strong>Corollary</strong>: The matrix $\textbf I - \gamma \textbf P \Pi$ is invertible</li> </ul> </li> <li> <strong>Remark</strong>: Bootstrapping update</li> </ul> </li> <li> <strong>Optimal value function</strong>: $V^* (s) = \max_a \mathbb E_{s' \sim P(.|s, a)}[ r(s,a,s') + \gamma V^* (s')]$</li> <li> <strong>Optimal action-value function</strong>: $Q^* (s,a) = \mathbb E_{s' \sim P(.|s, a)}[r(s,a,s') + \gamma \max_{a'} Q^* (s', a')]$ <ul> <li> <strong>Remark</strong>: Do not care about the transition tuples, how actions are selected, because it should satisfy Bellman equation for all possible transitions</li> </ul> </li> </ul> </li> <li> <strong>Occupancy measure</strong>: stationary distribution over $\mathcal S \times \mathcal A$ or $\mathcal A$ space, induced by running policy $\pi$ in the environment. <ul> <li> <strong>State-action</strong>: $\rho(s,a)$ <ul> <li> <strong>Unnormalized version</strong>: $\hat \rho^\pi(s,a) = \sum_{t=0}^\infty \gamma^t P(s_t = s, a_t = a| \rho_0, \pi, P)$ <ul> <li> <strong>Matrix format</strong>: $(\textbf I - \gamma \textbf P \Pi)^{-1} \in [0,1]^{|\mathcal S | |\mathcal A | \times |\mathcal S | |\mathcal A |}$</li> </ul> </li> <li> <strong>Normalized version</strong>: $\rho^\pi(s,a) = \frac{\hat \rho^\pi(s,a)}{\sum_{s, a}\hat \rho^\pi(s,a)} = (1-\gamma) \hat \rho^\pi (s,a)$</li> </ul> </li> <li> <strong>State</strong>: $\rho(s)$ <ul> <li> <strong>Unnormalized version</strong>: $\hat \rho^\pi(s) = \sum_{t=0}^\infty \gamma^t P(s_t = s| \rho_0, \pi, P)$ <ul> <li> <strong>Matrix format</strong>: $(\textbf I - \gamma \Pi \textbf P)^{-1} \in [0,1]^{|\mathcal S | \times |\mathcal S |}$</li> </ul> </li> <li> <strong>Normalized version</strong>: $\rho^\pi(s) = \frac{\hat \rho^\pi(s)}{\sum_{s}\hat \rho^\pi(s)} = (1-\gamma) \hat \rho^\pi (s)$</li> </ul> </li> <li> <strong>Lemma</strong>: 2 policies $\pi$ and $\pi'$ are equivalent iff they have the same $V^\pi(s) = V^{\pi'}(s) \forall s \in \mathcal S$ or $\rho^\pi(s,a) = \rho^{\pi'}(s,a), \forall s\in\mathcal S,a\in\mathcal A$</li> <li> <strong>Lemma</strong>: For all $\pi$, there always exists a stationary policy $\pi'$ with the same occupancy measure. <ul> <li> <strong>Corollary</strong>: It is sufficed to consider stationary policy.</li> </ul> </li> <li> <strong>Expected return</strong>:</li> <li> <strong>Value function</strong>:</li> <li> <strong>Action-value function</strong>:</li> </ul> </li> <li> <strong>Advantage functions</strong>: $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$ <ul> <li> <strong>Definition</strong>: How much an action is better than others</li> <li> <strong>Remark</strong>: It is easier to learn the consequence of an action is better than the other, than it is to learn the actual return from taking actions</li> </ul> </li> <li> <strong>Models</strong> <ul> <li> <strong>Model-based</strong>: learn $P(.|s_t. a_t)$ and $r$, sample efficiency, planning, sometimes no ground-truth model, bias leads to suboptimal performance, still challenge in model-learning <ul> <li> <strong>Pure planning</strong>: model predictive control to select actions, not policy, MBMF</li> <li> <strong>Expert iteration</strong>: learn explicit representation of $\pi_\theta(a|s)$, planning (MCTS) and generate actions, AlphaZero</li> <li> <strong>Data augmentation</strong>: use fictitious and/or real experience to learn $\pi(.|s)$ or $Q^\pi$, MBVE, World Models</li> <li> <strong>Embedding planning loops</strong>: embed planning into policy as a subrountine, l2A</li> </ul> </li> <li> <strong>Model-free</strong>: Easy to implement, tune, not sample efficiency <ul> <li> <strong>Policy optimization</strong>: learn approximator $\pi_\theta(a|s)$ for $\pi(a|s)$, directly or indirectly optimize $J(\pi_\theta)$, on-policy (most updated version of policy), usually involves learning $V_\theta(s)$ for $V^\pi(s)$, A2C, A3C, PPO</li> <li> <strong>Q-Learning</strong>: learn approximator $Q_\theta(s,a)$ for $Q^*$, off-policy (data collected at any point during training), DQN, C51</li> <li> <strong>Trade-off</strong>: direct vs indirect optimization, stable vs sample efficiency.</li> <li> <strong>Interpolation</strong>: under some circumstances $\rightarrow$ equivalent, DDPG, SAC</li> </ul> </li> </ul> </li> <li> <strong>Prediction vs Control</strong> <ul> <li> <strong>Prediction</strong>: <ul> <li> <strong>Input</strong>: MDP or MRP and $\pi$</li> <li> <strong>Output</strong>: $V^{\pi}$ or $Q^{\pi}$</li> </ul> </li> <li> <strong>Control</strong>: <ul> <li> <strong>Input</strong>: MDP or MRP</li> <li> <strong>Output</strong>: $V^{*}$ or $Q^{*}$</li> </ul> </li> </ul> </li> </ul> <h2 id="bandit">Bandit</h2> <h2 id="dynamic-programming">Dynamic Programming</h2> <ul> <li>Optimal substructure + Overlapping subproblems</li> <li> <strong>Algorithm</strong>: Policy iteration, value iteration</li> <li> <strong>Synchronous vs Asynchronous</strong> (in-place, prioritised sweeping, real-time)</li> <li> <strong>Properties</strong>: full-width backups, effective for medium-sized problems (millions of states)</li> <li> <strong>Drawback</strong>: Computationally expensive and known MDP</li> <li> <strong>Policy iteration</strong>: Policy evaluation + Policy improvement <ul> <li> <strong>Policy evaluation</strong>: $V^{k+1}(s) = \mathbb E_{a\sim \pi(.|s),s'\sim P(.|s, a)} [r(s,a,s') + \gamma V^{k}(s')]$ (iterative) <ul> <li> <strong>Matrix format</strong>: $\textbf V^{k+1} = \Pi\textbf r + \gamma\Pi\textbf P\textbf V^{k} $</li> </ul> </li> <li> <strong>Policy improvement</strong>: $\pi’= $greedy($V^{\pi}$) = $\arg\max_a Q^\pi(s|a)$ (greedy) <ul> <li>Consider two policy $\pi(a|s)$ and $\pi'(a|s)$, if $\foralll s \in \mathcal S$, we have that $Q^\pi(s,\pi') \ge V^\pi(s)$, then it holds $V^{\pi'}(s) \ge V^\pi(s), \forall s\in\mathcal S$. Then $\pi'$ is at least as good as $\pi$.</li> </ul> </li> <li> <ul> <li> <strong>Theorem</strong>: Policy Iteration generates a sequence of policies with non-decreasing value function. When the MDP is finite, the convergence occurs in a finite number of iterations.</li> </ul> </li> <li> <strong>Bellman expectation operator for policy $\pi$</strong>: $\mathcal T^\pi: \mathbb R^{|S|} \rightarrow \mathbb R^{|S|}$ that satisfies $\forall V \in\mathbb R^{|\mathcal S|}$, $(\mathcal T^\pi V)(s) = \mathbb E_{a\sim \pi(.|s),s'\sim P(.|s, a)} [r(s,a,s') + \gamma V^{\pi}(s')]]$ <ul> <li> <strong>Remark</strong>: $\mathcal T^\pi$ has the same property as $\mathcal T$</li> </ul> </li> </ul> </li> <li> <strong>Principle of Optimality</strong>: The policy $\pi$ can achieve optimal value from state s $V^*(s)$ if and only if any state $s'$ is reachable from s and $\pi$ achieve optimal value from state $s'$</li> <li> <strong>Value iteration</strong>: inspired by principle of optimality <ul> <li>$V^* (s) = \max_a \mathbb E_{s' \sim P(.|s, a)}[ r(s,a,s') + \gamma V^* (s')]$</li> <li> <strong>Bellman optimality operator:</strong> $\mathcal T: \mathbb R^{|\mathcal S|} \rightarrow \mathbb R^{|\mathcal S|}$ that satisfies $\forall V \in\mathbb R^{|\mathcal S|}$, $(\mathcal T V)(s) = \max_a \mathbb E_{s' \sim P(.| s,a)}[r(s,a', s') + \gamma V(s') ]$</li> <li> <strong>Properties:</strong> <ul> <li>$V_{k+1} = \mathcal T V_k$</li> <li>$V^* = \mathcal T V^*$</li> <li>For all $U, V\in\mathbb R^{|\mathcal S|}$, there exists $\gamma \in[0,1)$ such that $\mathcal T$ is a contraction mapping, \(\|\mathcal T U-\mathcal T V \|_{\infty} \le \| U - V \|_{\infty}\)</li> <li>$\lim_{k\rightarrow \infty V_k} = V^*$ (unique fixed point)</li> </ul> </li> <li> <strong>Remark</strong>: No explicit policy like policy iteration</li> </ul> </li> </ul> <h2 id="monte-carlo">Monte Carlo</h2> <h2 id="temporal-difference">Temporal Difference</h2> <h2 id="learningplanning-dyna-q">Learning/Planning Dyna-Q</h2> <h2 id="function-approximation">Function approximation</h2> <h2 id="policy-optimization">Policy optimization</h2> <ul> <li> <strong>Assumption</strong>: Finite-horizon undiscounted return</li> <li> <strong>Policy gradient algorithms</strong>: Derive analytical gradient $\rightarrow$ Form a sample estimate of expected return</li> <li> <strong>Formula</strong>: $\theta_{k+1} = \theta_{k} + \alpha \nabla_\theta J(\pi_\theta)|_{\theta_k}$</li> </ul> <p>\(\begin{aligned}\nabla_\theta J(\pi_\theta) &amp;= \nabla_\theta \mathbb E_{\tau\sim\pi_\theta}[R(\tau)] \\ &amp;= \nabla_\theta \int_\tau P(\tau|\pi_\theta)R(\tau)\\ &amp;= \int_\tau \nabla_\theta P(\tau|\pi_\theta)R(\tau)\\ &amp;= \int_\tau P(\tau|\pi_\theta)\nabla_\theta \log P(\tau|\pi_\theta)R(\tau) &amp; \text{Log derivative trick}\\ &amp;= \int_\tau P(\tau|\pi_\theta)\sum_{t=0}^T\nabla_\theta \log \pi_\theta(a_t|s_t)R(\tau) &amp; \text{Only }\pi\text{ depends on } \theta\\ &amp;= \mathbb E_{\tau\sim\pi_\theta}[\sum_{t=0}^T\nabla_\theta \log \pi_\theta(a_t|s_t)R(\tau) ]\\ &amp;= \mathbb E_{\tau\sim\pi_\theta}[\sum_{t=0}^T\nabla_\theta \log \pi_\theta(a_t|s_t)\sum_{t'=t}^T R(s_{t'}, a_{t'}, a_{t'+1}) ] &amp; \text{Reward-to-go}\\ &amp;= \mathbb E_{\tau\sim\pi_\theta}[\sum_{t=0}^T\nabla_\theta \log \pi_\theta(a_t|s_t)(\sum_{t'=t}^T R(s_{t'}, a_{t'}, a_{t'+1}) - b(s_t)) ] &amp; \text{Baseline in PG}\\ &amp;= \mathbb E_{\tau\sim\pi_\theta}[\sum_{t=0}^T\nabla_\theta \log \pi_\theta(a_t|s_t)\Phi_t ] &amp; \text{Baseline in PG} \end{aligned}\)</p> <ul> <li> <strong>Reward-to-go</strong>: Previous rewards does not influence the current decision, Reduces variance, improves sample efficiency.</li> <li> <strong>Expected Grad-Log-Prob Lemma:</strong> $\mathbb E_{\tau\sim\pi_\theta}[\nabla_\theta \log \pi_\theta(a_t|s_t)] = 0$</li> <li> <strong>Baseline in Policy Gradient</strong>: Inspired by Expected Grad-Log-Prob Lemma, $b(s_t)$ does not depend on $\theta$ ($V^{\pi}(s_t)$), reduces variance.</li> <li> <strong>Remark</strong>: $\Phi_t$ can be full reward, reward-to-go, reward-to-go minus the baseline, $Q^{\pi_\theta}(s_t,a_t)$, $A^{\pi_\theta}(s_t,a_t)$</li> <li> <strong>Remark</strong>: Data distribution depends on the generated policy, which depends on $\theta$</li> <li> <strong>Remark</strong>: Loss function performing well does not mean improving expected return. Only care about average return, not loss.</li> <li> <strong>Remark</strong>: $V^{\pi}(s_t)$ is approximated by NN $V_\phi(s_t)$, and updated concurrently with the policy <ul> <li> <strong>Objective</strong>: $\phi_k = argmin_{\phi} \mathbb E_{s_t, \hat R_t \sim\pi_k}[(V_\phi(s_t) - \hat R_t)^2 ]$ (epoch k)</li> </ul> </li> </ul> <h2 id="deep-rl">Deep RL</h2> <ul> <li> <strong>Key factors</strong>: reliability, sample efficiency</li> <li> <strong>On-policy</strong>: <ul> <li>VPG $\rightarrow$ TRPO $\rightarrow$ PPO</li> <li>Trade-off sample efficiency (not use old data) and reliability (optimize directly policy performance)</li> </ul> </li> <li> <strong>Off-policy</strong>: <ul> <li>DDPG $\rightarrow$ TD3 $\rightarrow$ SAC</li> <li>Exploit Bellman’s equations $\rightarrow$ satisfy any environment interaction data</li> <li>No guarantees to have better policy performance.</li> </ul> </li> </ul> <h3 id="vpg">VPG</h3> <ul> <li>on-policy algorithm, not much exploration, use in either discrete and continuous action space</li> <li> <strong>Objective (To update $\theta$, then $\phi$)</strong>:</li> </ul> \[\theta_{k+1} = \theta_k + \alpha_k\frac{1}{|D|}\sum_{\tau\in D}\sum_{t=0}^T\nabla_{\theta} \log \pi_\theta(a_t|s_t)\|_{\theta_k}\hat A_t\] \[\phi_{k+1} = \arg\min_\phi \frac{1}{|D|T}\sum_{\tau\in D}\sum_{t=0}^T (V_\phi(s_t) - \hat R_t)^2\] <h3 id="trpo">TRPO</h3> <ul> <li>On-policy algorithm, the largest possible step that satisfy the constraint on how close new and old policies using KL divergence.</li> <li> <strong>Objective</strong>:</li> </ul> \[\begin{aligned} \theta_{k+1} &amp;= \arg\max_\theta L(\theta_k, \theta) &amp; \text{ s.t }D_{KL}(\theta||\theta_k) \le \delta\\ &amp;= \arg\max_\theta \mathbb E_{s,a\sim\theta_{\theta_k}}[\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)] &amp; \text{ s.t }E_{s\sim\pi_{\theta_k}}[D_{KL}(\pi_\theta(.|s)||\pi_{\theta_k}(.|s))] \le \delta\\ &amp;= \arg\max_\theta g^T(\theta-\theta_k) &amp; \text{ s.t } \frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k) \le \delta \\ &amp;= \theta_k + \sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g &amp; \text{Lagrangian duality}\\ &amp;= \theta_k + \alpha^j\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g &amp; \text{Backtracking}\\ &amp;= \theta_k + \alpha^j\sqrt{\frac{2\delta}{g^T\hat{x}}}\hat{x} &amp; \text{Expensive computation } H^{-1} \end{aligned}\] <h3 id="ppo">PPO</h3> <ul> <li>Similar idea to TRPO, but relaxed objective. The distance between old policy and new policy is constrained by $1+\epsilon$ \(\theta_{k+1} = \arg\max_\theta \mathbb E_{s,a \sim\pi_{\theta_k}}[L(s,a,\theta_k,\theta)]\)</li> </ul> \[\begin{aligned} L(s,a,\theta_k,\theta) &amp; = \min (\frac{\pi_\theta(a|s)}{\pi_{\theta_k}}(a|s)A^{\pi_{\theta_k}(s,a)}, clip(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, 1-\epsilon, 1+\epsilon)A^{\pi_{\theta_k}}(s,a)) \\ &amp;= \min (\frac{\pi_\theta(a|s)}{\pi_{\theta_k}}(a|s)A^{\pi_{\theta_k}(s,a)}, g(\epsilon, A^{\pi_{\theta_k}}(s,a)) \end{aligned}\] <h3 id="ddpg">DDPG</h3> <ul> <li>Off-policy (replay buffer contains data generated by outdated policy), continuous action spaces, concurrently learns Q-function (Bellman equation and off-policy data) and $\pi$ (Q-function)</li> <li> <strong>Assumption</strong>: $Q^*(s,a)$ is differentiable with respect to a</li> <li> <strong>Goal</strong>: Learns $Q_\phi(s,a)$ to approximate $Q^*(s,a) = \mathbb E_{s'\sim P} [ r(s,a) + \gamma max_{a'} Q^{*} (s',a')]$</li> <li> <strong>Objective</strong>:</li> </ul> \[\begin{aligned}L(\phi,D) &amp;= \mathbb E_{s,a,r,s',d\sim D}[(Q_\phi(s,a) - (r + \gamma(1-d)Q_{\phi_{targ}}(s',\mu_{\theta_{targ}}(s'))))^2] \\ &amp;\approx \frac{1}{B}\sum_{(s,a,r,s',d) \in B} (Q_\phi(s,a) - (r + \gamma(1-d)Q_{\phi_{targ}}(s',\mu_{\theta_{targ}}(s'))))^2 \end{aligned}\] \[E_{s\in D}[Q_\phi(s,\mu_\theta(s))] \approx \frac{1}{B} \sum_{s\in B}Q_\phi(s,\mu_\theta(s))\] <ul> <li> <strong>Replay Buffer</strong>: Large enough for stability, impossible to contain everything, trade-off between recent data (overfitting) and old data (slow)</li> <li> <strong>Target Networks</strong>: Target is also parameterized by $\phi, \theta \rightarrow$ unstable training, parameterized as $\phi_{targ}, \theta_{targ}$, copy of main network for every fixed number of steps, updated as $\phi_{targ} = \rho\phi_{targ} + (1-\rho)\phi, \theta_{targ} = \rho\theta_{targ} + (1-\rho)\theta$</li> <li> <strong>Issues</strong>: Hyperparameter tuning, drastically overestimate Q-value (exploit Q-function errors)</li> </ul> <h3 id="twin-delayed-ddpg">Twin Delayed DDPG</h3> <ul> <li>Off-policy, continuous action spaces</li> <li> <strong>Clipped Double-Q Learning</strong>: Learns 2 Q-functions, Uses the smaller ones as the target, reduce further the overestimation</li> <li> <strong>Delayed Policy Updates</strong>: Update policy less frequently than Q-function, reduce the volatility when policy update changes the target</li> <li> <strong>Target Policy Smoothing</strong>: Added noise to action (form of regularization)</li> </ul> \[\begin{aligned} Q_{\phi_{targ}}(s',a'(s')) &amp;= Q_{\phi_{targ}}(s', clip(\mu_{\theta_{targ}}(s') + clip(\epsilon,-c,c), a_{Low}, a_{High}))) &amp; \epsilon\sim N(0,\sigma) \end{aligned}\] <h3 id="sac">SAC</h3> <ul> <li>Both continuous and discrete action spaces</li> <li> <strong>Entropy regularized RL</strong>: how random a random variable is, $H(P) = \mathbb E_{x\sim P}[-\log P(x)]$, more reward for high entropy (exploration)</li> <li> <strong>Similar to TD3</strong>: Q-functions with MSBE, target Q-networks (polyak averaging), clipped double-Q</li> <li> <strong>Different from TD3</strong>: entropy regularization, stochastic policy replaces target policy smoothing, next state-action comes from current policy instead of target policy (replay buffer)</li> <li> <strong>Remark</strong>: The objective is no longer the $Q^*$</li> </ul> \[\begin{aligned} Q^\pi(s,a) &amp;= \mathbb E_{s'\sim P, a'\sim \pi}[R(s,a,s') + \gamma(Q^\pi(s',a') + \alpha H(\pi(.|s')))] &amp; \text{Regularized entropy}\\ &amp;= \mathbb E_{s'\sim P, a'\sim \pi}[R(s,a,s') + \gamma(Q^\pi(s',a') - \alpha \log\pi(a'|s'))] \\ &amp;= r + \gamma (Q^\pi(s',\tilde a') - \alpha \log\pi(\tilde a'|s')) &amp; \text{Current policy, not from replay buffer} \end{aligned}\] <p>\(\begin{aligned} V^\pi(s,a) &amp;= E[Q^\pi(s,a) -\alpha \log\pi(a|s)] \\ &amp;= E_{s\in D,\delta\sim\mathcal N}[Q^\pi(s,\tilde a_\theta(s,\delta)) -\alpha \log\tilde a_\theta(s,\delta)] &amp; \tilde a_\theta(s,\delta) = \tanh(\mu_\theta(s) + \sigma_\theta(s) \odot \delta)\\ \end{aligned}\)</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://spinningup.openai.com/en/latest/user/algorithms.html" rel="external nofollow noopener" target="_blank">https://spinningup.openai.com/en/latest/user/algorithms.html</a></li> <li><a href="https://arxiv.org/pdf/2301.01379.pdf" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2301.01379.pdf</a></li> <li><a href="http://incompleteideas.net/book/RLbook2020.pdf" rel="external nofollow noopener" target="_blank">http://incompleteideas.net/book/RLbook2020.pdf</a></li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"chuducthang77/chuducthang77.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Thang Duc Chu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: March 01, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>