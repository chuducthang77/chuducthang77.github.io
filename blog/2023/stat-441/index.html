<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Statistical learning | Thang Duc Chu</title> <meta name="author" content="Thang Duc Chu"> <meta name="description" content="A note of my class STAT 441 - Statistical methods for learning and data mining"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://chuducthang77.github.io/blog/2023/stat-441/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Thang </span>Duc Chu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Statistical learning</h1> <p class="post-meta">January 30, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/stat"> <i class="fa-solid fa-hashtag fa-sm"></i> stat</a>     ·   <a href="/blog/category/stat-441"> <i class="fa-solid fa-tag fa-sm"></i> STAT-441</a>   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ml</a>   <a href="/blog/category/stat"> <i class="fa-solid fa-tag fa-sm"></i> stat</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="statistical-learning">Statistical Learning</h1> <h2 id="terminology">Terminology</h2> <ul> <li> <strong>Input</strong>: predictors, independent variables, features, variables</li> <li> <strong>Output</strong>: responses, dependent variables</li> </ul> <h2 id="prediction">Prediction</h2> <ul> <li>Predict \(\hat Y = \hat f(X)\)</li> <li>The accuracy of \(\hat Y\) depends on the <em>reducible error</em> and <em>irreducible error</em> </li> <li> <strong>Reducible error</strong>: The difference between \(\hat f\) and true \(f\)</li> <li> <strong>Irreducible error</strong>: \(f\) also depends on \(\epsilon\), unmeasurable, unknown</li> </ul> \[\mathbb E[(Y - \hat Y)^2] = (f(X) - \hat f(X))^2 + \text{Var}[\epsilon]\] <h2 id="inference">Inference</h2> <ul> <li>Which predictors are associated with Y?</li> <li>What is the relationship between Y and each predictor?</li> <li>Is a linear equation adequate to represent the relationship between Y and predictors?</li> </ul> <h2 id="how-to-estimate-f">How to estimate f</h2> <ul> <li> <strong>Parametric</strong>: Make <strong>assumption about functional form</strong>, uses training data to fit the model, easier to estimate a set of parameters, \(\hat f\) may be far from \(f\).</li> <li> <strong>Non-parametric</strong>: No assumption about functional form, estimate f that close to data points without being too rough or wiggly, requires a very large number of observations.</li> </ul> <h2 id="accuracy-vs-interpretability-vs-flexibility">Accuracy vs Interpretability (vs Flexibility)</h2> <ul> <li>More flexible = (maybe) more accurate = Less interpretability</li> </ul> <h2 id="assess-model-accuracy">Assess model accuracy</h2> <ul> <li> <strong>No Free Lunch theorem</strong>: no method dominates all others over all possible data sets.</li> <li> <strong>Measuring the Quality of Fit</strong>: MSE for regression, error rate for classification.</li> <li> <strong>Training vs Testing error</strong>: Monotonic decreasing and U-shape, training error is always smaller since we directly optimize.</li> <li> <strong>Overfitting vs Underfitting</strong>: U-shape testing error, both training and testing error are high.</li> </ul> <h2 id="bias-variance-trade-off">Bias-Variance Trade-off</h2> \[\mathbb E[(y_0 - \hat f(x_0))^2] = \text{Var}[\hat f(x_0)] + \text{Bias}(\hat f(x_0))^2 + \text{Var}[\epsilon]\] <ul> <li> <strong>Remark</strong>: Expected MSE cannot lie below \(Var[\epsilon]\)</li> <li> <strong>Remark</strong>: We want to achieve <strong>simultaneously</strong> low bias and low variance</li> <li> <strong>Variance</strong>: How \(\hat f\) changes if we shift one of the data point</li> <li> <strong>Bias</strong>: Error introduced by approximating a real-life problem <ul> <li> <strong>Remark</strong>: High flexible = High variance = Less bias</li> <li> <strong>Remark</strong>: The rate of changing bias and variance matters</li> </ul> </li> </ul> <h2 id="the-bayes-classifier">The Bayes classifier</h2> <ul> <li>Assigns each observation to most likely class given its predictor values</li> <li> <strong>Bayes error rate</strong>: \(1 - \max_j \Pr(Y = j \|X = x_0)\), analogous to irreducible error <ul> <li>Gold standard to compare other methods</li> </ul> </li> <li> <p><strong>K-nearest neighbor</strong>: Unknown \(P(Y = j \| X= x_0)\)</p> \[\max_{j} \Pr(Y = j \| X = x_0 ) = \frac{1}{k}\sum_{i\in N_0} I(y_i = j)\] <ul> <li> <strong>Remark</strong>: Close to optimal Bayes classifier</li> <li> <strong>Remark</strong>: Selecting K matters (bias-variance trade-off)</li> </ul> </li> </ul> <h1 id="linear-regression">Linear Regression</h1> <h2 id="question">Question</h2> <ul> <li>Is there a relationship between response and predictors?</li> <li>Is the relationship linear?</li> <li>How strong the relationship is?</li> <li>Is there an interaction (synergy) effect among predictors?</li> <li>How accurately our prediction is?</li> </ul> <h2 id="formulation">Formulation</h2> \[Y = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p + \epsilon\] <ul> <li> <strong>Assumption</strong>:\(\epsilon\):\(\perp\!\!\!\!\perp X\) , \(\epsilon_i \perp\!\!\!\!\perp \epsilon_j\), catch-all we miss (nonlinear relationship, missing predictors, measurement error)</li> <li> <strong>Residual</strong>: \(e_i = y_i - \hat y_i\)</li> <li> <strong>Objective</strong>: \(\min RSS = e_1^2 + ... + e_n^2\)</li> <li> <strong>Estimating coefficients</strong>: Least squares solution</li> <li> <strong>Interpretation</strong>: <ul> <li>\(\hat\beta_0\): Average value of Y</li> <li>\(\hat\beta_j\): Average increase in Y if we increase \(X_j\) by 1</li> </ul> </li> </ul> <h2 id="simple-linear-regression">Simple Linear Regression</h2> <ul> <li> <p>Number of predictor \(p = 1\)</p> </li> <li> <p><strong>Assessing model</strong>:</p> <ul> <li> <p><strong>Residual standard error</strong>: measure lack of fit, an estimate of standard deviation of \(\epsilon\)</p> \[\hat\sigma = RSE = \sqrt{\frac{RSS}{n-p-1}}\] </li> <li> <p><strong>\(R^2\)-statistics</strong>: absolute measure of lack of fit, proportion of variability explained by regression</p> \[0 \le R^2 = 1 - \frac{RSS}{TSS} \le 1\] <ul> <li> <strong>RSS</strong>: amount of variability left after performing regression</li> <li> <strong>TSS</strong>: total variance in response Y - amount of variability before regressing (only \(\bar y\))</li> <li>Interpretational advantage over RSE, but what is <em>good</em> \(R^2\)?</li> <li> <strong>Remark</strong>: \(R^2 = r^2\) (correlation)</li> </ul> </li> <li> <p><strong>Others</strong>: confidence interval, hypothesis testing, p-value</p> </li> </ul> </li> </ul> <h2 id="multiple-linear-regression">Multiple Linear Regression</h2> <ul> <li>Number of predictor \(p &gt; 1\)</li> <li> <strong>Is there relationship between response and predictors?</strong>: Hypothesis testing (F-test) <ul> <li>All predictors</li> <li> <strong>Linearity assumption satisfies</strong>: \(E[\frac{RSS}{n-p-1}] = \sigma^2\)</li> <li> <strong>\(H_0\)</strong>: \(E[\frac{RSS}{n-p=1}]=\sigma^2\)</li> <li> <strong>Remark</strong>: Test all together, instead of individual</li> </ul> </li> <li> <strong>Which predictors are important?</strong> <ul> <li> <strong>Criteria</strong>: Mallow’s \(C_p\), AIC, BIC, Adjusted-\(R^2\)</li> <li> <strong>Procedure</strong>: Forward selection, backward selection, or mixed</li> </ul> </li> <li> <strong>How good model fits?</strong>: RSE and \(R^2\)-statistics <ul> <li> <strong>Remark</strong>: The relative change between p and RSS</li> <li> <strong>Remark</strong>: More variables always increase \(R^2\) (overfitting)</li> </ul> </li> <li> <strong>How accurate the prediction?</strong> <ul> <li> <strong>Reducible error</strong>: Inaccuracy between \(f(X)\) and \(\hat Y\), model bias <ul> <li> <strong>Confidence interval</strong>: interval contains the true value \(f(X)\), uncertainty around average Y over large X</li> </ul> </li> <li> <strong>Irreducible error</strong>: Inaccuracy between \(Y\) and \(\hat Y\), random error \(\epsilon\) <ul> <li> <strong>Prediction interval</strong>: interval contains the true value of Y, uncertainty around Y over a particular X</li> </ul> </li> </ul> </li> </ul> <h2 id="qualitative-response-variable">Qualitative response variable</h2> <ul> <li> <strong>Dummy variable</strong>: incorporating qualitative variable into regression analysis</li> <li> <strong>Coding scheme 0/1</strong> (One-hot encoding): <ul> <li>\(\beta_0\): Average Y when X = 0</li> <li>\(\beta_1\): The difference in average Y when X = 0 and X = 1</li> </ul> </li> <li> <strong>Coding scheme -1/1</strong>: <ul> <li>\(\beta_0\): Average Y (ignore X)</li> <li>\(\beta_1\): Amount each X have Y that above or below average</li> </ul> </li> <li> <strong>Remark</strong>: Coding scheme does not affect the fit</li> <li> <strong>More than 2 level</strong>: Always one fewer dummy variable than number of levels <ul> <li>No dummy variable = baseline</li> </ul> </li> </ul> <h2 id="extension-of-linear-model">Extension of linear model</h2> <ul> <li> <strong>Remove additive assumption</strong>: <ul> <li>Increase in \(X_j\) associated with one-unit increase in \(X_k\)</li> <li> <strong>Hierarchical principle</strong>: If we include the <strong>interaction term</strong>, we also should <strong>include the main effect</strong> even if the p-value associated with the coefficients are <strong>not significant</strong>.</li> </ul> </li> <li> <strong>Nonlinear relationship</strong>: Polynomial regression</li> </ul> <h2 id="potential-problems">Potential Problems</h2> <ul> <li> <strong>Nonlinear between response and predictors</strong> <ul> <li> <strong>Residual plot</strong>: Observe any discerning patterns</li> <li> <strong>Solution</strong>: transformation on X</li> </ul> </li> <li> <strong>Correlation among error terms</strong> <ul> <li>Underestimate the true standard error (Accidentally double the observations)</li> <li> <strong>Very</strong> importance to linear regression</li> </ul> </li> <li> <strong>Non-constant variance of error term</strong> <ul> <li> <strong>Residual plot</strong>: Funnel shape in residual plot</li> <li> <strong>Solution</strong>: transformation on Y</li> </ul> </li> <li> <strong>Outliers</strong>: Point where \(y_i\) is unusual given \(x_i\) <ul> <li> <strong>Remark</strong>: Outliers can have no effect on least square fit</li> <li> <strong>Studentized residuals</strong>: Detect outliers</li> </ul> </li> <li> <strong>High-leverage points</strong>: Point where \(x_i\) is unusual <ul> <li> <strong>Remark</strong>: Removing high leverage points are important</li> <li> <strong>Remark</strong>: High-leverage points are outliers, but not vice versa</li> <li> <strong>Leverage statistics</strong>: To detect high leverage points!</li> </ul> \[h_i = \frac{1}{n} + \frac{(x_i - \bar x)^2}{\sum_{i'=1}^n (x_{i'} - \bar x)^2}\] <ul> <li>Between 1 and \(\frac{1}{n}\) and the average value <strong>ALWAYS</strong> equals \(\frac{p+1}{n}\)</li> </ul> </li> <li> <strong>Collinearity</strong>: Two or more variables are closely related to each other <ul> <li>Difficult to separate out the individual effects</li> <li>Inaccurate \(\hat\beta_j\) and \(SE[\hat\beta_j]\) increases, power of the hypothesis test reduces</li> <li> <strong>Correlation matrix</strong>: Good for pairs of variables</li> <li> <strong>Multicollinearity</strong>: Variance inflation factor</li> <li> <strong>Solution</strong>: Drop or combine collinear variables together into a single predictor/</li> </ul> </li> </ul> <h2 id="comparsion-with-knn">Comparsion with KNN</h2> <ul> <li>Parametric vs non-parametric</li> <li> <strong>Linearity assumption</strong>: LR</li> <li> <strong>Non-linearity in low dimension</strong>: LR</li> <li> <strong>Non-linearity in high dimension</strong>: KNN</li> <li> <strong>Small observations</strong>: LR (Curse of dimensionality)</li> </ul> <h1 id="logistic-regression">Logistic Regression</h1> <h2 id="why-not-linear-regression">Why not Linear Regression</h2> <ul> <li> <strong>Coding scheme</strong>: implies outcomes ordering.</li> <li> <strong>Linear Regression</strong>: Hard to interpret as probability, and impossible for more than 2 classes</li> </ul> <h2 id="logistic-model">Logistic Model</h2> <ul> <li> <strong>Logistic function</strong>: \(p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\)</li> <li> <strong>Odds</strong>: \(\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X}\)</li> <li> <strong>Logit odds</strong>: \(\log \frac{p(X)}{1-p(X)} = \beta_0 + \beta_1 X\) <ul> <li>\(\beta_1\): Increases X by 1 changes the <strong>logit odds</strong> by \(\beta_1\).</li> </ul> </li> </ul> <h2 id="estimating-coefficients">Estimating coefficients</h2> <ul> <li> <strong>Maximum likelihood function</strong>:</li> </ul> \[l(\beta_0, \beta_1) = \prod_{i:y_i = 1}p(x_i) \prod_{i':y_{i'}=0} (1 - p(x_{i'}))\] <ul> <li> <strong>Predictions</strong>: Possible to have qualitative predictors (using dummy variables)</li> </ul> <h3 id="multiple-logistic-regression">Multiple Logistic Regression</h3> \[p(Y=1\|X) = p(X) = \frac{e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}\] <h3 id="multinomial-logistic-regression">Multinomial Logistic Regression</h3> <ul> <li> <strong>Baseline encoding</strong>:</li> </ul> \[P(Y = k \| X = x) = \frac{e^{\beta_{k0} + \beta_{k1} X_1 + ... + \beta_{kp} X_p}}{1 + \sum_{l=1}^{K-1}e^{\beta_{l0} + \beta_{l1} X_1 + ... + \beta_{lp} X_p}}, \quad \forall k = 1,...,K-1\] \[P(Y = k \| X = x) = \frac{1}{1 + \sum_{l=1}^{K-1}e^{\beta_{l0} + \beta_{l1} X_1 + ... + \beta_{lp} X_p}}, \quad k = K\] <ul> <li> <p><strong>Remark</strong>: The choice of baseline will <strong>change the coefficients</strong>, but the fitted value will be the same.</p> </li> <li> <p><strong>Softmax encoding</strong>: No baseline</p> </li> </ul> \[P(Y = k \| X = x) = \frac{e^{\beta_{k0} + \beta_{k1} X_1 + ... + \beta_{kp} X_p}}{\sum_{l=1}^{K}e^{\beta_{l0} + \beta_{l1} X_1 + ... + \beta_{lp} X_p}}\] <h2 id="generative-models-for-classification">Generative Models for classification</h2> <h3 id="motivation">Motivation</h3> <ul> <li> <strong>Condition</strong>: Substantial separation between 2 classes, small sample size, predictors are approximately normal.</li> </ul> \[p_k(x) = P(Y = k \| X= x) = \frac{P(Y = k)P(X = x\|Y = k)}{\sum_{l=1}^K P(Y = l) P(X =x \| Y = l)} = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}\] <ul> <li> <strong>Challenge</strong>: estimating \(f_k(x) \rightarrow\) simplifying assumption</li> </ul> <h3 id="linear-discriminant-analysis-p--1">Linear Discriminant Analysis p = 1</h3> <ul> <li> <strong>Assumption</strong>: \(f_k(x) \sim N(\mu_k, \sigma_k^2)\) and \(\sigma_1^2 = ... = \sigma_K^2 = \sigma^2\)</li> </ul> \[\max_k p_k(x) =\delta_k(x) = x \times\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)\] <ul> <li> <strong>Remark</strong>: Linear in terms of x</li> <li> <strong>Challenge</strong>: \(\mu_k\) and \(\sigma^2\) are unknown. Then, estimate</li> </ul> \[\hat\mu_k = \frac{1}{n_k}\sum_{i:y_i = k}x_i\] \[\hat\sigma^2 = \frac{1}{n-K}\sum_{k=1}^K\sum_{i:y_i = k}(x_i - \hat\mu_k)^2\] \[\hat\pi_k = \frac{n_k}{n}\] <h3 id="linear-discriminant-analysis-p--1-1">Linear Discriminant Analysis p &gt; 1</h3> <ul> <li> <strong>Assumption</strong>: \(f_k(x)\sim N(\mu_k, \Sigma)\)</li> </ul> \[\max_k p_k(x) = \delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log\pi_k\] <ul> <li> <strong>Decision boundary</strong>: \(\delta_k(x) = \delta_l(x)\)</li> <li> <strong>Remark</strong>: Prefer when more than 2 classes (view data in low dimension)</li> </ul> <h3 id="metric">Metric</h3> <ul> <li><strong>Confusion matrix</strong></li> <li> <strong>Sensitivity</strong>: Percent of TP</li> <li> <strong>Specificity</strong>: Percent of TN <ul> <li>Bayes classifier does poorly in Specificity since minimizing <em>total</em> error rate</li> <li> <strong>Threshold</strong> (in K = 2) affects specificity and sensitivity \(\rightarrow\) domain knowledge</li> </ul> </li> <li> <strong>ROC curve</strong>: Overall performance of the classifier, all possible threshold, given by area under curve (AUC) <ul> <li> <strong>False positive rate</strong> = 1 - Specificity = Type I error</li> <li> <strong>True positive rate</strong> = Sensitivity = Recall = 1 - Type II error = Power</li> <li> <strong>Precision</strong> = Among positive prediction, how many actually TP</li> </ul> </li> </ul> <h3 id="quadratic-discriminant-analysis">Quadratic Discriminant Analysis</h3> <ul> <li> <strong>Assumption</strong>: \(f_k(x)\sim N(\mu_k, \Sigma_k)\)</li> </ul> \[\max_k p_k(x) = \delta_k(x) = -\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k - \frac{1}{2}\log\|\Sigma_k\| + \log\pi_k\] <ul> <li> <strong>Challenge</strong>: High variance \(\rightarrow\) work better with more observations (bias-variance trade-off)</li> </ul> <h3 id="naive-bayes">Naive Bayes</h3> <ul> <li> <strong>Assumption</strong>: Among kth class, p predictors are independent, \(f_k(x) = f_{k1}(x_1) \times... \times f_{kp}(x_p)\)</li> <li> <strong>Remark</strong>: Works well when n &lt; p (reduces variance)</li> </ul> \[P(Y = k \| X = x) = \frac{\pi_k \times f_{k1}(x_1) \times ...\times f_{kp}(x_p)}{\sum_{l=1}^K \pi_l \times f_{l1}(x_1) \times ... \times f_{lp}(x_p)}\] <ul> <li>If \(X_j\) is quantitative, \(f_{kj}(x_j) \sim N(\mu_{kj}, \sigma^2_{kj})\) (Similar to QDA with diagonal covariance matrix) or nonparametric (kernel density estimator)</li> <li>If \(X_j\) is qualitative, count the proportion of training obs for jth predictor corresponding to each class</li> </ul> <h2 id="comparison-of-all-methods">Comparison of all methods</h2> <ul> <li>NB takes a form generalized additive model</li> <li>LDA is a special case of QDA</li> <li>Any linear boundary classifiers are special cases of NB with \(g_{kj} = b_{kj}x_j\)</li> <li>NB when \(X_j\) is quantitative and \(f_{kj}(x_j) \sim N(\mu_{kj}, \sigma^2_{kj})\) are special case of QDA (\(\Sigma\) is a diagonal matrix)</li> <li>LDA will be better than Logistic Regression when Normal assumption does not hold.</li> <li>KNN is better when decision boundary is highly non-linear and n » p. If the relationship is non-linear and n \(\approx\) p, QDA is prefer.</li> </ul> <h2 id="generalized-linear-model">Generalized Linear Model</h2> <ul> <li>Y belongs to <strong>Exponential Family Distribution</strong> </li> <li> <strong>Poisson Regression</strong>: \(\lambda(X_1,...,X_p) = e^{\beta_0 + \beta_1X_1 + ... + \beta_pX_p}\) and use MLE to find \(\hat\beta\) <ul> <li> <strong>Interpretation</strong>: Increases \(X_j\) by one unit with a change in \(\mathbb E[Y]\) by \(\exp(\beta_j)\)</li> <li> <strong>Mean variance relationship</strong>: \(Var[Y] = \mathbb E[Y] = \lambda\)</li> <li> <strong>Nonnegative</strong> fitted values</li> </ul> </li> <li> <strong>Link function</strong>: Transform mean of the response so that the transformed mean is a linear function of predictors</li> </ul> <h1 id="sampling-method">Sampling Method</h1> <ul> <li> <strong>Model selection</strong>: Select proper level of flexibility</li> <li> <strong>Model assessment</strong>: Estimate the test error</li> </ul> <h2 id="cross-validation">Cross-validation</h2> <h3 id="validation-set-approach">Validation Set approach</h3> <ul> <li>Randomly divided train and valdiation set</li> <li> <strong>Challenge</strong>: Overestimate and highly variable test error rate</li> <li>High bias (overestimation), High variance</li> </ul> <h3 id="loocv">LOOCV</h3> <ul> <li> <p>Leave one as validation and the rest as training set. Repeat n times</p> \[CV_{(n)} = \frac{1}{n}\sum_{i=1}^n MSE_{i}\] </li> <li> <strong>Benefits</strong>: Unbiased estimate of test error rate, less variance and bias in train/valid split.</li> <li> <strong>Challenge</strong>: expensive</li> <li> <strong>Remark</strong>: Less expensive for LSS or polynomial regression (exact solution)</li> </ul> <h3 id="k-fold-cv">k-fold CV</h3> <ul> <li> <p>Randomly divide the dataset into k groups</p> \[CV_{(k)} = \frac{1}{k}\sum_{i=1}^k MSE_i\] </li> <li>LOOCV is a <strong>special</strong> case</li> <li> <strong>Benefits</strong>: Less expensive, more accurate estimation of test error rate</li> <li> <strong>Challenge</strong>: More variance (more random in split), more bias (less in training)</li> <li> <strong>Classification</strong>: Similar idea, but can sometimes underestimate the test error rate!</li> </ul> <h2 id="bootstrap">Bootstrap</h2> <ul> <li>Apply when difficult to obtain a measure of variability.</li> <li> <strong>Problem</strong>: Data cannot be generated from original population.</li> <li> <strong>Solution</strong>: <strong>Repeated sampling (with replacement)</strong> observations from the <em>original</em> dataset \(\rightarrow\) <strong>obtain the estimation</strong> and <strong>use the formula</strong> to obtain the standard error of the estimation.</li> </ul> <h1 id="regularization">Regularization</h1> <h2 id="motivation-1">Motivation</h2> <ul> <li>Instead of using LS model</li> <li> <strong>Model interpretability</strong>: Irrelevant variables leads to unnecessary complexity, which makes harder to interpret.</li> </ul> <h2 id="subset-selection">Subset selection</h2> <ul> <li> <strong>Best Subset selection</strong>: \(2^p - 1\) models <ul> <li>For each k (0,…, p-1) predictors \(\rightarrow\) Fit \(p - k\) models with one more predictors \(\rightarrow\) Choose best model (smallest RSS or highest \(R^2\), or deviance), called \(M_k\) \(\rightarrow\) Choose the best model among \(M_k\) (k=1,…,p) (cross-validation error, \(C_p\), BIC, or adjusted-\(R^2\))</li> <li> <strong>Challenge</strong>: expensive</li> </ul> </li> </ul> <h2 id="stepwise-selection">Stepwise selection</h2> <ul> <li> <strong>Motivation</strong>: Large search space \(\rightarrow\) overfitting and high variance in the estimate</li> <li> <strong>Forward</strong>: <ul> <li>For each k (1,…, p) predictors \(\rightarrow\) Fit \(p - k\) models with one additional models \(\rightarrow\) Choose best model (smallest RSS or highest \(R^2\), or deviance), called \(M_k\) \(\rightarrow\) Choose the best model among \(M_k\) (k=1,…,p) (cross-validation error, \(C_p\), BIC, or adjusted-\(R^2\))</li> <li>Do well in <strong>practice</strong> </li> <li> <strong>Not guarantee</strong> to find best model</li> <li>Possible to apply when n &lt; p</li> </ul> </li> <li> <strong>Backward</strong>: <ul> <li>For each k (p, p-1, …, 1) predictors \(\rightarrow\) Fit \(k\) models with one less predictor than previously \(\rightarrow\) Choose best model (smallest RSS or highest \(R^2\), or deviance), called \(M_k\) \(\rightarrow\) Choose the best model among \(M_k\) (k=1,…,p) (cross-validation error, \(C_p\), BIC, or adjusted-\(R^2\))</li> <li>Requires n &gt; p</li> </ul> </li> <li> <strong>Hybrid</strong>: Combination of two previous ones</li> </ul> <h2 id="choosing-the-optimal-model"><strong>Choosing the optimal model</strong></h2> <ul> <li> <strong>Indirectly estimate testing error</strong>: adjust training error <ul> <li>Assumption about true underlying model</li> <li>\(C_p\): \(\frac{1}{n}(RSS + 2d\hat\sigma^2)\) <ul> <li>\(\hat\sigma^2\): estimate of \(\epsilon\) (full-model)</li> <li>More features (d) will increase \(C_p\)</li> <li>\(E[C_p]\) = Test MSE (unbiased)</li> </ul> </li> <li>\(AIC\): \(\frac{1}{n}(RSS + 2d\hat\sigma^2)\) <ul> <li>Class of models fit by MLE</li> <li>Proportional to \(C_p\)</li> </ul> </li> <li>\(BIC\): \(\frac{1}{n}(RSS + \log(n)d\hat\sigma^2)\) <ul> <li>Bayesian POV</li> <li>Heavier penalty for large model</li> </ul> </li> <li>Adjusted-\(R^2\): \(1\frac{RSS/(n-d-1)}{TSS/(n-1)}\) <ul> <li>Adding more features reduces RSS, and reduces (n-d-1). Therefore, the relative change matters.</li> <li>Not well motivated in statistical theory</li> <li>Large value is better (Opposite with above three criterion)</li> </ul> </li> </ul> </li> <li> <strong>Directly estimate testing error</strong>: validation set approach or cross-validation <ul> <li>Fewer assumption about true underlying model</li> <li>Prefer when it is hard to pinpoint df and \(\sigma^2\)</li> </ul> </li> </ul> <h2 id="shrinkage">Shrinkage</h2> <ul> <li> <strong>Ridge regression</strong>:</li> </ul> \[RSS + \lambda\sum_{j=1}^p \beta_j^2\] <ul> <li>\(\lambda\): shrinkage penalty, shrink \(\beta_j\) toward zero</li> <li> <strong>Remark</strong>: shrinkage penalty is not applied to \(\beta_0\) (measure of the mean value of the response)</li> <li> <strong>Remark</strong>: \(\lambda\) increases may increase estimated coefficients</li> <li> <strong>Remark</strong>: LS solution is scale equivariant, while RR coeff is not.</li> <li> <strong>Remark</strong>: Standardizing features before applying ridge regression</li> <li> <strong>Bias-variance trade-off</strong>: Increase bias, reduce variance (flexibility)</li> <li> <strong>Remark</strong>: Shrink coefficients towards, but not exactly, zero</li> <li>\(\min_\beta \|\|Y - X\beta\|\|^2\) subject to \(\sum_{j=1}^p \beta_j^2 \le s\)</li> <li> <p><strong>Remark</strong>: Normal distribution prior and follows by posterior mode/mean for \(\beta\)</p> </li> <li> <strong>LASSO</strong>:</li> </ul> \[RSS + \lambda\sum_{j=1}^p \|\beta_j\|\] <ul> <li> <strong>Model interpretation/Variable selection</strong>: Forcing some coefficients to be exactly zero</li> <li> <strong>Sparse model</strong>: \(\lambda\) is sufficiently large</li> <li>\(\min_\beta \|Y - X\beta\|^2\) subject to \(\sum_{j=1}^p \|\beta_j\| \le s\)</li> <li> <strong>Closely related to Best Subset selection</strong>: \(\min_\beta \|Y - X\beta\|^2\) subject to \(\sum_{j=1}^p I(\beta_j\not = 0) \le s\)</li> <li> <strong>Remark</strong>: Corner solution</li> <li> <strong>Remark</strong>: Neither LASSO nor Ridge regression universally dominates the other</li> <li> <strong>Remark</strong>: Since the derivative of absolute function does not exist at 0, we need <strong>soft thresholding</strong> (explicitly set coefficients to be 0)</li> <li> <strong>Remark</strong>: Laplace distribution prior and follows posterior mode (not mean)</li> </ul> <h2 id="dimension-reduction-to-be-written-in-the-near-future">Dimension reduction: To be written in the near future</h2> <h2 id="consideration-in-high-dimension-to-be-written-in-the-near-future">Consideration in high dimension: To be written in the near future</h2> <h1 id="beyond-linearity">Beyond Linearity</h1> <h2 id="polynomial-regression"><strong>Polynomial regression</strong></h2> \[y_i = \beta_0 + \beta_1 x_i + ... +\beta_d x_i^d +\epsilon\] <ul> <li>Extremely non-linear curve</li> <li> <strong>Remark</strong>: d &lt; 5, otherwise overly flexible and strange shapes</li> <li> <strong>Variance of the fit</strong>: point-wise square-root of the variance estimate for each coefficients</li> <li>Applicable for linear and logistic regression</li> </ul> <h2 id="step-functions"> <strong>Step functions</strong>:</h2> \[y_i = \beta_0 + \beta_1 C_1(x_i) + ... + \beta_K C_K(x_i) + \epsilon\] <ul> <li>Global structure on the non-linear function</li> <li>Continuous variable \(\rightarrow\) ordered categorical variable</li> <li>\(C_K(x) = I(c_K \le x \le c_{K+1})\): indicator function, dummy variable</li> <li>\(\beta_0\): Mean value of Y for \(X &lt; c_1\)</li> <li>\(\beta_j\): Average increase in Y for X in \(c_j \le X &lt; c_{j=1}\) relative to \(X &lt; c_1\)</li> <li> <strong>Remark</strong>: Unless natural breakpoints, miss the action</li> </ul> <h2 id="basis-functions"> <strong>Basis functions</strong>:</h2> \[y_i = \beta_0 + \beta_1 b_1(x_i) + ... + \beta_K b_K(x_i) + \epsilon\] <ul> <li>\(b_k\): fixed and known function (Step and polynomial regression are special case of this)</li> <li>Applicable for OLS</li> </ul> <h2 id="regression-splines"> <strong>Regression splines</strong>:</h2> <ul> <li> <strong>Piecewise Polynomials</strong>: \(y_i = \beta_{01} + \beta_{11}x_i + \beta_{21}x_i^2 + \epsilon\) if \(x_i &lt; c\) otherwise \(\beta_{02} + \beta_{12} x_i + \beta_{22}x_i + \epsilon\) <ul> <li> <strong>Knots</strong>: c, more knots = more flexible</li> <li> <strong>Remark</strong>: Discontinuous (too flexible)</li> <li> <strong>Remark</strong>: 1 knot and 4 parameters \(\rightarrow\) 8 parameters in total</li> </ul> </li> <li> <strong>Constraints</strong>: first up to K-1 order derivative must be continuous <ul> <li> <strong>Remark</strong>: Every constraint frees one degree of freedom</li> <li> <strong>Remark</strong>: Cubic splines has K (knots) + 4 (\(\beta_0, \beta_1, \beta_2, \beta_3\)) degrees of freedom</li> </ul> </li> <li> <strong>Spline Basis Representation</strong>: <ul> <li> <strong>Truncated Power basis</strong> (cubic spline): \(h(x, \xi) = (x - \xi)_+^3\) with \(\xi\) is the location of the knot</li> <li> <strong>Remark</strong>: Discontinuity in third derivative</li> <li> <strong>Remark</strong>: Higher variance at outer range</li> </ul> </li> <li> <strong>Natural spline</strong>: Additional linearity boundary constraints</li> <li> <strong>Choosing the number and locations of the knots</strong> <ul> <li> <strong>Remark</strong>: More knots = more flexible = coefficients change rapidly</li> <li><strong>Uniform fashion</strong></li> </ul> </li> <li> <strong>Compare with Polynomial Regression</strong> <ul> <li>Different ways to introduce flexibility \(\rightarrow\) more stable</li> </ul> </li> </ul> <h2 id="smoothing-splines"> <strong>Smoothing splines</strong>:</h2> <p>g = \(\arg\min_g \sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2dt\)</p> <ul> <li> <strong>Remark</strong>: g is <strong>VERY</strong> flexible (interpolates all \(y_i\) makes RSS = 0) - <strong>Roughness</strong>: second derivative = how fast first derivative change - <strong>Remark</strong>: Integration = total change in the first derivative - \(\lambda\): \(\lambda \rightarrow \infty\), g becomes very smooth - <strong>Remark</strong>: shrunken version of natural cubic spline - <strong>Remark</strong>: As \(\lambda\) increases \(0 \rightarrow \infty\), \(df_{\lambda}\) decreases \(n\rightarrow 2\) - <strong>Effective degrees of freedom</strong>: measure the flexibility of the spline</li> </ul> <h2 id="local-regression"> <strong>Local regression</strong>:</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Compute the fit at target point using nearby training observations
- $$K_{i0}$$ different for each value of $$x_0
- **Memory-based**: Similar to KNN (need neighbor to compute)
- **s**: span, proportion of points to compute local regression, similar to $$\lambda
  - **Remark**: smaller = more wiggly
- Poor performance in high dimension (Curse of dimensionality)
</code></pre></div></div> <h2 id="generalized-additive-models-to-be-written-in-the-future"> <strong>Generalized additive models</strong>: To be written in the future</h2> <h1 id="tree-based">Tree-based</h1> <ul> <li><strong>Basics of Decision Trees</strong></li> <li> <strong>Regression Trees</strong> <ul> <li><strong>Stratification of Feature Space</strong></li> <li><strong>Tree Pruning</strong></li> </ul> </li> <li><strong>Classification Trees</strong></li> <li><strong>Trees vs Linear models</strong></li> <li><strong>Advantages and Disadvantages</strong></li> <li> <strong>Bagging, RF, Boosting, Bayesian Additive Regression Trees</strong> <ul> <li> <strong>Bagging</strong> <ul> <li><strong>Out-of-Bag error estimation</strong></li> <li><strong>Variable Important measures</strong></li> </ul> </li> <li><strong>RF</strong></li> <li><strong>Boosting</strong></li> <li> <strong>Bayesian Additive Regression Trees</strong>: To be added in the future</li> </ul> </li> </ul> <h1 id="svm">SVM</h1> <ul> <li> <strong>Maximal Margin Classifier</strong> <ul> <li><strong>Hyperplane</strong></li> <li><strong>Classification using separating hyperplane</strong></li> <li><strong>The classifier</strong></li> <li><strong>Construction of the classifier</strong></li> <li><strong>Non-separable cases</strong></li> </ul> </li> <li> <strong>Support Vector Classifier</strong> <ul> <li><strong>Overview</strong></li> <li><strong>Detail</strong></li> </ul> </li> <li> <strong>Support Vector Machines</strong> <ul> <li><strong>Non-linear Decision Boundaries</strong></li> <li> <strong>More than 2 cases</strong> <ul> <li><strong>One vs one</strong></li> <li><strong>One vs all</strong></li> </ul> </li> <li><strong>Relationship to logistic regression</strong></li> </ul> </li> </ul> <h1 id="neural-network">Neural Network</h1> <h1 id="unsupervised-learning">Unsupervised Learning</h1> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Thang Duc Chu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: April 10, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>